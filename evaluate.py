"""
HotpotQA è¯„ä¼°è„šæœ¬ï¼ˆå…¨é‡å»ºå›¾ + é™å®šæ£€ç´¢èŒƒå›´æ¨¡å¼ï¼‰

å¯¹æ ‡ KG2RAG è®ºæ–‡çš„å®éªŒè®¾ç½®ï¼š
- ç¦»çº¿æ„å»ºå…¨é‡çŸ¥è¯†å›¾è°±ï¼ˆæ‰€æœ‰å”¯ä¸€æ–‡æ¡£ï¼‰
- è¯„æµ‹æ—¶é€šè¿‡ doc_filter é™åˆ¶æ£€ç´¢èŒƒå›´åˆ°æ¯ä¸ªé—®é¢˜çš„ 10 ä¸ªæ–‡æ¡£

ä½¿ç”¨æ–¹æ³•ï¼š
1. å…ˆè¿è¡Œå…¨é‡å»ºå›¾è„šæœ¬ï¼š
   python scripts/build_hotpot_global_kg.py --input data/hotpot_dev_distractor_v1.json --persist_dir ./data/hotpotqa --reset
   
2. å†è¿è¡Œè¯„æµ‹ï¼š
   python evaluate.py
"""
import json
import os
import time
import sys
import warnings

# --- å±è”½çƒ¦äººçš„ transformers è­¦å‘Š ---
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

# --- è§£å†³ KMeans è­¦å‘Š ---
os.environ["OMP_NUM_THREADS"] = "1"

from typing import List, Dict, Any
from dataclasses import dataclass

# å¼•å…¥æ¨¡å—åŒ–å¼•æ“
from src.engine import AdvancedRAGEngine
# å¼•å…¥æ—¥å¿—å·¥å…·
from util.custom_logger import ExperimentLogger
# å¼•å…¥é…ç½®
from src.config import DEFAULT_BEAM_WIDTH, DEFAULT_MAX_HOPS
# ä½¿ç”¨å®˜æ–¹è¯„æµ‹è„šæœ¬ç»Ÿä¸€é€»è¾‘
from util.hotpot_evaluate_v1 import f1_score as calculate_metrics, normalize_answer


@dataclass
class MinRAGInput:
    id: str
    query: str
    documents: List[Dict[str, str]]
    answer_ground_truth: str
    supporting_facts_ground_truth: List[List[Any]]


class HotpotQALoader:
    def __init__(self, file_path: str, logger=None):
        self.file_path = file_path
        self.logger = logger
        self.data = []

    def log(self, msg):
        if self.logger:
            self.logger.info(msg)
        else:
            print(msg)

    def load(self):
        self.log(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†: {self.file_path} ...")
        if not os.path.exists(self.file_path):
            self.log(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {self.file_path}")
            return

        with open(self.file_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        self.log(f"âœ… æˆåŠŸåŠ è½½ {len(self.data)} æ¡æ•°æ®ã€‚")

    def process_sample(self, raw_sample: Dict) -> MinRAGInput:
        """
        å°† HotpotQA çš„åŸå§‹æ•°æ®è½¬æ¢ä¸º Engine éœ€è¦çš„æ ¼å¼ã€‚
        """
        documents = []
        for item in raw_sample['context']:
            title = item[0]
            sentences = item[1]
            text_content = ' '.join(sentences)
            documents.append({
                "title": title,
                "text": text_content
            })

        return MinRAGInput(
            id=raw_sample['_id'],
            query=raw_sample['question'],
            documents=documents,
            answer_ground_truth=raw_sample['answer'],
            supporting_facts_ground_truth=raw_sample['supporting_facts']
        )

    def get_batch(self, batch_size: int = 1):
        for i in range(0, len(self.data), batch_size):
            batch_raw = self.data[i: i + batch_size]
            yield [self.process_sample(sample) for sample in batch_raw]


# ==========================================
# ğŸš€ æ‰§è¡Œä¸»ç¨‹åº
# ==========================================
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
    logger = ExperimentLogger(log_dir=os.path.join(PROJECT_ROOT, "logs"), experiment_name="kgprag_eval")
    
    # ä½¿ç”¨ç»å¯¹è·¯å¾„å¼•ç”¨æ•°æ®
    DATA_FILE = os.path.join(PROJECT_ROOT, "data", "hotpot_dev_distractor_v1.json")
    OUTPUT_FILE = os.path.join(PROJECT_ROOT, "data", "advanced_rag_results.jsonl")
    # å…¨é‡å›¾è°±ç´¢å¼•ç›®å½•ï¼ˆéœ€ä¸ scripts/build_hotpot_global_kg.py çš„ --persist_dir ä¿æŒä¸€è‡´ï¼‰
    INDEX_DIR = os.path.join(PROJECT_ROOT, "data", "hotpotqa")

    # 2. æ£€æŸ¥ç´¢å¼•ç›®å½•
    logger.info("\n" + "=" * 80)
    logger.info("ğŸš€ æ­£åœ¨å¯åŠ¨ Advanced GraphRAG å¼•æ“ (å…¨é‡å»ºå›¾ + é™å®šæ£€ç´¢æ¨¡å¼)...")
    logger.info(f"   ğŸ“‚ ç´¢å¼•ç›®å½•: {INDEX_DIR}")
    logger.info("   âœ… å¯¹æ ‡ KG2RAG è®ºæ–‡å®éªŒè®¾ç½®")
    logger.info("   âœ… å…¨é‡å›¾è°± + doc_filter é™åˆ¶æ£€ç´¢èŒƒå›´")
    logger.info("   âœ… å¤šä¿¡å·å¯ä¿¡åº¦è¯„åˆ†: Enabled")
    logger.info("=" * 80 + "\n")
    
    # æ£€æŸ¥ç´¢å¼•ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(INDEX_DIR) or not os.path.exists(os.path.join(INDEX_DIR, "doc_cache.json")):
        logger.error(f"âŒ ç´¢å¼•ç›®å½•ä¸å­˜åœ¨æˆ–ç´¢å¼•æœªæ„å»º: {INDEX_DIR}")
        logger.error("è¯·å…ˆè¿è¡Œå…¨é‡å»ºå›¾è„šæœ¬:")
        logger.error("  python scripts/build_hotpot_global_kg.py --input data/hotpot_dev_distractor_v1.json --persist_dir ./index --reset")
        exit(1)
    
    # åŠ è½½æ ·æœ¬-æ–‡æ¡£æ˜ å°„
    mapping_path = os.path.join(INDEX_DIR, "sample_doc_mapping.json")
    if not os.path.exists(mapping_path):
        logger.error(f"âŒ æ ·æœ¬-æ–‡æ¡£æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {mapping_path}")
        logger.error("è¯·é‡æ–°è¿è¡Œå…¨é‡å»ºå›¾è„šæœ¬")
        exit(1)
    
    with open(mapping_path, "r", encoding="utf-8") as f:
        sample_doc_mapping = json.load(f)
    logger.info(f"ğŸ“ åŠ è½½æ ·æœ¬-æ–‡æ¡£æ˜ å°„: {len(sample_doc_mapping)} ä¸ªæ ·æœ¬")

    try:
        # ä½¿ç”¨æŒä¹…åŒ–æ¨¡å¼åŠ è½½å…¨é‡å›¾è°±
        engine = AdvancedRAGEngine(persist_dir=INDEX_DIR, online_mode=True)
    except Exception as e:
        logger.error(f"âŒ å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        exit()

    # 3. åŠ è½½æ•°æ®
    loader = HotpotQALoader(DATA_FILE, logger=logger)
    loader.load()
    if not loader.data:
        logger.error("âŒ æ•°æ®æœªåŠ è½½ï¼Œç¨‹åºé€€å‡ºã€‚")
        exit()

    # 4. é…ç½®æµ‹è¯•å‚æ•°ï¼ˆBeam/Hops ä½¿ç”¨ config é»˜è®¤å€¼ï¼‰
    MAX_TESTS = 7405
    BEAM_WIDTH = DEFAULT_BEAM_WIDTH
    MAX_HOPS = DEFAULT_MAX_HOPS

    count = 0
    total_f1 = 0.0
    total_precision = 0.0
    total_recall = 0.0
    retrieval_hit_count = 0
    bridge_hit_count = 0
    path_hit_count = 0

    logger.info(f"\nğŸ¯ å¼€å§‹è¯„æµ‹ (è·‘ {MAX_TESTS} æ¡æ•°æ®)")
    logger.info(f"âš™ï¸ å‚æ•°: Beam Width={BEAM_WIDTH}, Hops={MAX_HOPS}")
    logger.info(f"ğŸ’¾ ç»“æœå°†å®æ—¶ä¿å­˜åˆ°: {OUTPUT_FILE}\n")

    # æ¸…ç©ºæ—§çš„ç»“æœæ–‡ä»¶
    if os.path.exists(OUTPUT_FILE):
        os.remove(OUTPUT_FILE)

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f_out:

        for batch in loader.get_batch(batch_size=1):
            for item in batch:
                count += 1
                logger.info(f"{'-' * 60}")
                logger.info(f"è¿›åº¦: [{count}/{MAX_TESTS}] | Query ID: {item.id}")
                logger.info(f"â“ é—®é¢˜: {item.query}")

                start_time = time.time()
                try:
                    # --- Step A: è·å–è¯¥æ ·æœ¬çš„æ–‡æ¡£è¿‡æ»¤å™¨ (HotpotQA-Dist è®¾ç½®) ---
                    doc_filter = None
                    if item.id in sample_doc_mapping:
                        doc_filter = set(sample_doc_mapping[item.id])
                        logger.info(f"ğŸ“‹ doc_filter: {len(doc_filter)} ä¸ªæ–‡æ¡£")
                    else:
                        logger.warning(f"âš ï¸ æ ·æœ¬ {item.id} æœªæ‰¾åˆ°æ–‡æ¡£æ˜ å°„ï¼Œä½¿ç”¨å…¨åº“æ£€ç´¢")

                    # --- Step B: å¤šè·³æ¨ç†ï¼ˆä½¿ç”¨ doc_filter é™åˆ¶æ£€ç´¢èŒƒå›´ï¼‰---
                    result = engine.query(
                        item.query,
                        beam_width=BEAM_WIDTH,
                        max_hops=MAX_HOPS,
                        doc_filter=doc_filter,
                        return_debug=True
                    )
                    if isinstance(result, tuple):
                        prediction, debug_info = result
                    else:
                        prediction, debug_info = result, {}

                    search_result = debug_info.get("search_result", {})
                    retrieved_nodes = search_result.get("nodes", [])
                    if item.supporting_facts_ground_truth:
                        supporting_titles = {sf[0] for sf in item.supporting_facts_ground_truth}
                    else:
                        supporting_titles = set()
                    retrieved_titles = {
                        n.get("doc_title") for n in retrieved_nodes if n.get("doc_title")
                    }
                    hit_titles = supporting_titles & retrieved_titles
                    support_needed = min(2, len(supporting_titles))

                    retrieval_hit = len(hit_titles) >= 1 if supporting_titles else False
                    bridge_hit = support_needed > 0 and len(hit_titles) >= support_needed

                    path_titles = set(search_result.get("best_path_doc_titles", []))
                    path_hit = support_needed > 0 and len(path_titles & supporting_titles) >= support_needed

                    if retrieval_hit:
                        retrieval_hit_count += 1
                    if bridge_hit:
                        bridge_hit_count += 1
                    if path_hit:
                        path_hit_count += 1

                    # --- Step D: è¯„ä¼° ---
                    f1, precision, recall = calculate_metrics(prediction, item.answer_ground_truth)

                    # --- Step E: LLM è¯­ä¹‰è£åˆ¤ ---
                    semantic_consistency = "N/A"
                    norm_pred = normalize_answer(prediction)
                    
                    if f1 < 0.8 and prediction.strip() and "i don't know" not in norm_pred:
                        try:
                            judge_prompt = f"""
                            Act as an objective judge. compare the Prediction and the Ground Truth.
                            Are they referring to the same entity, person, time, or event? 
                            Or is the Prediction a valid valid subset/synonym of the Ground Truth?
                            
                            Prediction: "{prediction}"
                            Ground Truth: "{item.answer_ground_truth}"
                            
                            Return ONLY 'yes' or 'no'.
                            """
                            from langchain_core.messages import HumanMessage
                            judge_res = engine.llm.invoke([HumanMessage(content=judge_prompt)]).content.lower().strip()
                            
                            if "yes" in judge_res:
                                semantic_consistency = "Consistent"
                                status_icon = "âœ… Sem-Match"
                            else:
                                semantic_consistency = "Different"
                        except Exception as e:
                            logger.error(f"Judge Error: {e}")

                    # åˆ¤å®šä¸å›¾æ ‡
                    if semantic_consistency != "Consistent":
                        if "i don't know" in norm_pred:
                            status_icon = "âšª IDK"
                        elif f1 >= 0.5:
                            status_icon = f"ğŸ‰ High F1 ({f1:.2f})"
                        elif f1 > 0:
                            status_icon = f"âš ï¸ Low F1 ({f1:.2f})"
                        else:
                            status_icon = "âŒ MISS"

                    duration = time.time() - start_time

                    logger.info(f"ğŸ¤– é¢„æµ‹: {prediction.strip()}")
                    logger.info(f"âœ… çœŸå€¼: {item.answer_ground_truth}")
                    if semantic_consistency == "Consistent":
                        logger.info(f"âš–ï¸ è£åˆ¤: âœ… è¯­ä¹‰ä¸€è‡´ (è™½ç„¶ F1={f1:.2f})")
                    logger.info(f"ğŸ“Š æŒ‡æ ‡: F1={f1:.2f} | P={precision:.2f} | R={recall:.2f} | {status_icon} (è€—æ—¶: {duration:.2f}s)")
                    logger.info(
                        f"ğŸ” Hits: retrieval={int(retrieval_hit)} "
                        f"bridge={int(bridge_hit)} path={int(path_hit)} "
                        f"| support={len(supporting_titles)} hit_docs={len(hit_titles)}"
                    )

                    # å†™å…¥æ–‡ä»¶
                    record = {
                        "id": item.id,
                        "query": item.query,
                        "prediction": prediction,
                        "ground_truth": item.answer_ground_truth,
                        "metrics": {
                            "f1": f1,
                            "precision": precision,
                            "recall": recall,
                            "semantic": semantic_consistency
                        },
                        "duration": duration,
                        "retrieval": {
                            "support_titles": sorted(supporting_titles),
                            "retrieved_title_count": len(retrieved_titles),
                            "hit_titles": sorted(hit_titles),
                            "retrieval_hit": retrieval_hit,
                            "bridge_hit": bridge_hit,
                            "path_hit": path_hit,
                        },
                        "method": "Advanced_GraphRAG_Modular"
                    }
                    f_out.write(json.dumps(record, ensure_ascii=False) + "\n")
                    f_out.flush()

                    # ç´¯è®¡
                    total_f1 += f1
                    total_precision += precision
                    total_recall += recall

                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å‡ºé”™: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

                if count >= MAX_TESTS:
                    break

            if count >= MAX_TESTS:
                break

    # 5. æ‰“å°æœ€ç»ˆç»Ÿè®¡
    logger.info(f"\n{'=' * 60}")
    logger.info(f"ğŸ æµ‹è¯•ç»“æŸ")
    if count > 0:
        logger.info(f"ğŸ“ˆ Avg F1:        {total_f1 / count:.4f}")
        logger.info(f"ğŸ“ˆ Avg Precision: {total_precision / count:.4f}")
        logger.info(f"ğŸ“ˆ Avg Recall:    {total_recall / count:.4f}")
        logger.info(f"ğŸ“Œ Retrieval Hit: {retrieval_hit_count / count:.4f} ({retrieval_hit_count}/{count})")
        logger.info(f"ğŸ“Œ Bridge Hit:    {bridge_hit_count / count:.4f} ({bridge_hit_count}/{count})")
        logger.info(f"ğŸ“Œ Path Hit:      {path_hit_count / count:.4f} ({path_hit_count}/{count})")
    else:
        logger.info("æ²¡æœ‰å¤„ç†ä»»ä½•æ•°æ®ã€‚")
    logger.info(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")
    logger.info(f"ğŸ“ å®Œæ•´æ—¥å¿—å·²ä¿å­˜è‡³: {logger.get_log_path()}")

    engine.close()
