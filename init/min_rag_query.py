"""
Min-RAG Query Engine (DeepSeek ç‰ˆ)
å®ç°åŠŸèƒ½ï¼šå‘é‡æ£€ç´¢ -> å›¾è°±å…³è”æ‰©å±• -> æœ€ç»ˆé—®ç­”
"""

import os
from typing import List, Set

# ä¿®å¤ Warningï¼Œä½¿ç”¨æ–°çš„åŒ…
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from neo4j import GraphDatabase

# ==============================================================================
# ğŸ”´ é…ç½® (ä¸å…¥åº“æ—¶ä¿æŒä¸€è‡´)
# ==============================================================================
os.environ["OPENAI_API_KEY"] = "sk-7b097abdf68f4e91ad414703d6e20f7a"
os.environ["OPENAI_BASE_URL"] = "https://api.deepseek.com"
LLM_MODEL = "deepseek-chat"  # é—®ç­”ä¹Ÿå¯ä»¥ç”¨ deepseek-reasoner (R1) æ•ˆæœæ›´å¥½

CHROMA_DIR = "./chroma_db_deepseek"
NEO4J_URI = "bolt://localhost:7687"
NEO4J_AUTH = ("neo4j", "9RP4s9YpWWSV:k3")
# ==============================================================================

def get_llm():
    return ChatOpenAI(model=LLM_MODEL, temperature=0.3)

def get_vector_store():
    print("[Init] åŠ è½½å‘é‡åº“...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/BGE-M3")
    return Chroma(persist_directory=CHROMA_DIR, embedding_function=embeddings)

def get_neo4j_driver():
    return GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)

# === æ ¸å¿ƒå‡½æ•° 1: å‘é‡æ£€ç´¢ (Hop 1) ===
def dense_retrieval(vector_store, query: str, top_k=1):
    print(f"\nğŸ” [Hop 1] å‘é‡æ£€ç´¢: '{query}'")
    docs = vector_store.similarity_search(query, k=top_k)

    results = []
    for doc in docs:
        chunk_id = doc.metadata.get("chunk_id")
        text = doc.page_content
        source = doc.metadata.get("source")
        print(f"   -> æ‰¾åˆ°: [{chunk_id}] æ¥è‡ª {source}")
        results.append({"id": chunk_id, "text": text, "source": source})
    return results

# === æ ¸å¿ƒå‡½æ•° 2: å›¾è°±æ‰©å±• (Hop 2) ===
def graph_expansion(driver, seed_chunks: List[dict]):
    """
    è®ºæ–‡æ ¸å¿ƒç®—æ³•ç®€åŒ–ç‰ˆï¼š
    Seed Chunks -> åŒ…å«çš„å®ä½“ -> è¿™äº›å®ä½“è¿æ¥çš„å…¶ä»– Chunks
    """
    print(f"\nğŸ•¸ï¸ [Hop 2] å›¾è°±æ‰©å±• (Looking for bridges...)")
    if not seed_chunks:
        return []

    expanded_chunks = []
    seed_ids = [c["id"] for c in seed_chunks]

    with driver.session() as session:
        # Cypher æŸ¥è¯¢é€»è¾‘ï¼š
        # 1. æ‰¾åˆ° Seed Chunk æåˆ°çš„å®ä½“ (e)
        # 2. æ‰¾åˆ°ä¹Ÿæåˆ°è¿™äº›å®ä½“ (e) çš„å…¶ä»– Chunk (other_c)
        # 3. è¿”å›è¿™äº› chunk çš„æ–‡æœ¬
        query = """
        MATCH (seed:Chunk)-[:MENTIONS]->(e:Entity)<-[:MENTIONS]-(other:Chunk)
        WHERE seed.id IN $seed_ids AND NOT other.id IN $seed_ids
        RETURN DISTINCT other.id, other.text, other.source, collect(e.name) as bridges
        LIMIT 3
        """
        result = session.run(query, seed_ids=seed_ids)

        for record in result:
            c_id = record["other.id"]
            text = record["other.text"]
            source = record["other.source"]
            bridges = record["bridges"]

            print(f"   -> æ‰©å±•åˆ°: [{c_id}] (æ¡¥æ¢å®ä½“: {bridges}) æ¥è‡ª {source}")
            expanded_chunks.append({"id": c_id, "text": text, "source": source})

    if not expanded_chunks:
        print("   -> (æ— å…³è”æ‰©å±•å†…å®¹)")

    return expanded_chunks

# === æ ¸å¿ƒå‡½æ•° 3: ç”Ÿæˆå›ç­” ===
def generate_answer(llm, query, context_chunks):
    print(f"\nğŸ¤– [Gen] æ­£åœ¨æ€è€ƒ...")

    # æ‹¼è£…ä¸Šä¸‹æ–‡
    context_str = ""
    for i, c in enumerate(context_chunks):
        context_str += f"--- æ–‡æ¡£ç‰‡æ®µ {i+1} (æ¥æº: {c['source']}) ---\n{c['text']}\n\n"

    prompt = f"""
è¯·åŸºäºä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
å¦‚æœå‚è€ƒä¿¡æ¯ä¸­æœ‰çŸ›ç›¾æˆ–ä¸åŒè§’åº¦çš„æè¿°ï¼Œè¯·ç»¼åˆåˆ†æã€‚

å‚è€ƒä¿¡æ¯ï¼š
{context_chunks}

ç”¨æˆ·é—®é¢˜ï¼š{query}

å›ç­”ï¼š
"""
    # æ‰“å°å®Œæ•´çš„ Prompt æ–¹ä¾¿è°ƒè¯• (å¯é€‰)
    # print(f"--- Prompt ---\n{prompt}\n----------------")

    response = llm.invoke(prompt)
    return response.content

def main():
    vs = get_vector_store()
    driver = get_neo4j_driver()
    llm = get_llm()

    # === æµ‹è¯•ç”¨ä¾‹ ===
    # è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„â€œå¤šè·³â€é—®é¢˜ï¼š
    # 1. å‘é‡æ£€ç´¢èƒ½æœåˆ°â€œAlpha CEOâ€æ˜¯å¼ ä¸‰ï¼Œå‘å¸ƒäº† SkyBrainã€‚
    # 2. ä½†æ˜¯â€œå¸‚åœºå‰æ™¯â€åœ¨å‘é‡åº“é‡Œå¾ˆéš¾ç›´æ¥åŒ¹é…ï¼ˆå› ä¸ºç ”æŠ¥é‡Œæ²¡æå¼ ä¸‰çš„åå­—ï¼‰ã€‚
    # 3. å›¾è°±æ‰©å±•é€šè¿‡ "SkyBrain" å®ä½“ï¼ŒæŠŠç ”æŠ¥æŠ“å–è¿›æ¥ï¼Œä»è€Œå›ç­”â€œå‰æ™¯ä¸ä½³â€ã€‚
    user_query = "Alpha å…¬å¸ CEO å‘å¸ƒçš„æœ€æ–°èŠ¯ç‰‡ï¼Œåœ¨å½“å‰çš„è¡Œä¸šåˆ†æä¸­é¢ä¸´ä»€ä¹ˆæ ·çš„å¸‚åœºå‰æ™¯ï¼Ÿ"

    # 1. Hop 1
    seed_chunks = dense_retrieval(vs, user_query, top_k=1)

    # 2. Hop 2
    expanded_chunks = graph_expansion(driver, seed_chunks)

    # 3. åˆå¹¶ä¸Šä¸‹æ–‡
    all_context = seed_chunks + expanded_chunks

    # 4. ç”Ÿæˆ
    answer = generate_answer(llm, user_query, all_context)

    print("\n" + "="*50)
    print(f"ç”¨æˆ·æé—®: {user_query}")
    print("-" * 50)
    print(f"Min-RAG å›ç­”:\n{answer}")
    print("="*50)

    driver.close()

if __name__ == "__main__":
    main()