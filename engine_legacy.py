"""
Advanced GraphRAG Engine (ç”Ÿäº§ç‰ˆ)
ç‰¹æ€§ï¼š
- ç»Ÿä¸€ä¸‰å±‚ KGï¼šDocument/Topic -> Chunk -> Entity
- æ˜¾å¼å†…å­˜æ¨¡å¼ï¼šä½¿ç”¨ chromadb.EphemeralClientï¼Œé¿å…æŒä¹…åŒ–/æ–‡ä»¶é”
- æ··åˆæ£€ç´¢ï¼šTopic/Chunk å‘é‡æ£€ç´¢ + å›¾æ‰©å±•ï¼ˆNEXT/RELATED/MENTIONSï¼‰
- åŒæ¨¡å‹æŠ½å–ï¼šGLiNER (å®ä½“) + REBEL (å…³ç³»)
"""

from __future__ import annotations

import os
import uuid
from typing import List, Dict, Set

import numpy as np
import torch
import chromadb
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from neo4j import GraphDatabase
from gliner import GLiNER
from FlagEmbedding import FlagReranker
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import re

# ================= REBEL Model Configuration =================
REBEL_MODEL = os.environ.get("REBEL_MODEL", "Babelscape/rebel-large")

# ================= Configuration =================
# ï¼ï¼ï¼è¯·åœ¨è¿™é‡Œå¡«å…¥æ‚¨çš„é…ç½®ä¿¡æ¯ï¼ï¼ï¼
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "sk-1408831cec78417d9a6024ac8e02dac4")
OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL", "https://api.deepseek.com")
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["OPENAI_BASE_URL"] = OPENAI_BASE_URL

LLM_MODEL = os.environ.get("LLM_MODEL", "deepseek-chat")
EMBED_MODEL = os.environ.get("EMBED_MODEL", "BAAI/bge-m3")
RERANKER_MODEL = os.environ.get("RERANKER_MODEL", "BAAI/bge-reranker-v2-m3")
GLINER_MODEL = os.environ.get("GLINER_MODEL", "urchade/gliner_medium-v2.1")

NEO4J_URI = os.environ.get("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.environ.get("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.environ.get("NEO4J_PASSWORD", "9RP4s9YpWWSV:k3")

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"[System] Running on device: {DEVICE}")


class AdvancedRAGEngine:
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ– AdvancedRAG å¼•æ“ (åŒæ¨¡å‹æŠ½å–: GLiNER + REBEL)...")

        self.llm = ChatOpenAI(model=LLM_MODEL, temperature=0)
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBED_MODEL,
            model_kwargs={"device": DEVICE},
            encode_kwargs={"normalize_embeddings": True},
        )

        # GLiNER: å®ä½“æŠ½å–
        print(f"ğŸ“¦ åŠ è½½å®ä½“æ¨¡å‹: {GLINER_MODEL}")
        self.entity_model = GLiNER.from_pretrained(GLINER_MODEL)
        if DEVICE == "cuda":
            self.entity_model.to("cuda")
        
        # REBEL: å…³ç³»æŠ½å–
        print(f"ğŸ“¦ åŠ è½½å…³ç³»æŠ½å–æ¨¡å‹: {REBEL_MODEL}")
        self.rebel_tokenizer = AutoTokenizer.from_pretrained(REBEL_MODEL)
        self.rebel_model = AutoModelForSeq2SeqLM.from_pretrained(REBEL_MODEL)
        if DEVICE == "cuda":
            self.rebel_model.to("cuda")
        self.rebel_model.eval()
        
        # Reranker
        print(f"ğŸ“¦ åŠ è½½é‡æ’æ¨¡å‹: {RERANKER_MODEL}")
        self.reranker = FlagReranker(RERANKER_MODEL, use_fp16=(DEVICE == "cuda"))

        self.driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
        self.chroma_client = None
        self.vector_store = None
        self.doc_cache: Dict[str, Dict] = {}

        self.reset()

    def _normalize_entity(self, entity_text: str) -> str:
        """
        [æ€è·¯B] å®ä½“åˆ«åå½’ä¸€åŒ–
        å»é™¤å† è¯ã€æ ‡ç‚¹ï¼Œç»Ÿä¸€å°å†™
        """
        # å»é™¤å¼€å¤´çš„å† è¯
        normalized = re.sub(r'^(the|a|an)\s+', '', entity_text.lower())
        # å»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™å­—æ¯æ•°å­—ç©ºæ ¼ï¼‰
        normalized = re.sub(r'[^\w\s]', '', normalized)
        return normalized.strip()

    def _extract_relations_rebel(self, text: str) -> List[Dict]:
        """
        ä½¿ç”¨ REBEL æ¨¡å‹æŠ½å–å…³ç³»ä¸‰å…ƒç»„ (head, relation, tail)
        REBEL æ˜¯ä¸“é—¨è®­ç»ƒçš„å…³ç³»æŠ½å–æ¨¡å‹ï¼Œæ”¯æŒ 200+ ç§å…³ç³»ç±»å‹
        """
        relations = []
        try:
            # æˆªæ–­è¿‡é•¿æ–‡æœ¬é¿å… OOM
            text_truncated = text[:512]
            
            # Tokenize
            inputs = self.rebel_tokenizer(
                text_truncated, 
                return_tensors="pt", 
                max_length=512, 
                truncation=True
            )
            if DEVICE == "cuda":
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # Generate
            with torch.no_grad():
                outputs = self.rebel_model.generate(
                    **inputs,
                    max_length=256,
                    num_beams=3,
                    num_return_sequences=1
                )
            
            # Decode
            decoded = self.rebel_tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]
            
            # Parse REBEL output format: <triplet> head <subj> relation <obj> tail
            relations = self._parse_rebel_output(decoded)
            
        except Exception as e:
            print(f"âš ï¸ REBEL Extraction Error: {e}")
        
        return relations
    
    def _parse_rebel_output(self, text: str) -> List[Dict]:
        """
        è§£æ REBEL è¾“å‡ºæ ¼å¼
        æ ¼å¼: <triplet> head <subj> relation <obj> tail <triplet> ...
        """
        relations = []
        
        # æ¸…ç†ç‰¹æ®Š token
        text = text.replace("<s>", "").replace("</s>", "").replace("<pad>", "")
        
        # æŒ‰ <triplet> åˆ†å‰²
        triplets = text.split("<triplet>")
        
        for triplet in triplets:
            triplet = triplet.strip()
            if not triplet:
                continue
            
            try:
                # æå– head
                if "<subj>" in triplet:
                    head = triplet.split("<subj>")[0].strip()
                    rest = triplet.split("<subj>")[1]
                else:
                    continue
                
                # æå– relation å’Œ tail
                if "<obj>" in rest:
                    relation = rest.split("<obj>")[0].strip()
                    tail = rest.split("<obj>")[1].strip()
                else:
                    continue
                
                # å½’ä¸€åŒ–å®ä½“å
                head_norm = self._normalize_entity(head)
                tail_norm = self._normalize_entity(tail)
                
                if head_norm and tail_norm and head_norm != tail_norm:
                    relations.append({
                        "source": head_norm,
                        "target": tail_norm,
                        "type": relation.upper().replace(" ", "_")
                    })
            except Exception:
                continue
        
        return relations

    def reset(self):
        """é‡ç½®ï¼šæ¸…ç©ºå›¾+å‘é‡åº“ï¼ˆçº¯å†…å­˜ EphemeralClientï¼‰"""
        self.doc_cache = {}
        try:
            with self.driver.session() as session:
                session.run("MATCH (n) DETACH DELETE n")
                session.run("CREATE INDEX chunk_id_idx IF NOT EXISTS FOR (c:Chunk) ON (c.id)")
                session.run("CREATE INDEX topic_id_idx IF NOT EXISTS FOR (t:Topic) ON (t.id)")
        except Exception as e:
            print(f"âš ï¸ Neo4j Reset Error: {e}")

        try:
            self.chroma_client = chromadb.EphemeralClient()
            unique_col = f"rag_mem_{uuid.uuid4().hex}"
            self.vector_store = Chroma(
                client=self.chroma_client,
                collection_name=unique_col,
                embedding_function=self.embeddings,
            )
        except Exception as e:
            print(f"âŒ Chroma Init Error: {e}")
            raise

    def _summarize_text(self, text: str, hint: str) -> str:
        """
        [ä¼˜åŒ–ç­–ç•¥] LLM å¢å¼ºæ‘˜è¦ (LLM-based Summary)
        çœŸæ­£çš„æ‘˜è¦æ ‘æ„å»ºéœ€è¦è¯­ä¹‰å‹ç¼©ï¼Œè€Œä¸ä»…ä»…æ˜¯æˆªæ–­ã€‚
        """
        if not text: return ""
        
        # ç®€å•çš„å¯å‘å¼å¿«é€Ÿè¿‡æ»¤ï¼ˆå¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œç›´æ¥è¿”å›ï¼ŒèŠ‚çœ Tokenï¼‰
        if len(text) < 500:
            clean_text = text.replace("\n", " ").strip()
            return f"[{hint}] " + clean_text

        try:
            # ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦
            from langchain_core.messages import HumanMessage
            prompt = f"Please provide a concise summary of the following text, focusing on the topic '{hint}'.\n\nText:\n{text[:4000]}"
            response = self.llm.invoke([HumanMessage(content=prompt)])
            return f"[{hint}] {response.content}"
        except Exception as e:
            print(f"âš ï¸ Summary LLM Error: {e}, falling back to heuristic.")
            clean_text = text.replace("\n", " ").strip()
            return f"[{hint}] " + clean_text[:300] + "..."

    

    def ingest(self, documents: List[Dict]):
        """
        ä¸‰å±‚å›¾è°±æ„å»ºï¼ˆæ‘˜è¦æ ‘ç‰ˆï¼‰ï¼š
        Layer 1 (Tree): Document -> Summary(L2) -> Summary(L1) -> Chunk
        Layer 2 (Passage): Chunk <-> Chunk (NEXT, RELATED)
        Layer 3 (Entity): Chunk -> Entity
        """
        if not documents:
            return

        lc_docs: List[Document] = []
        ids: List[str] = []
        all_chunks: List[Dict] = []

        # Step 1: Chunk/Embedding/Entity Extraction (No LLM)
        for i, doc in enumerate(documents):
            # ... (this part is unchanged) ...
            cid = f"chunk_{i}"
            text = doc.get("text", "")
            title = doc.get("title", "")

            self.doc_cache[cid] = {"text": text, "title": title}
            embedding = self.embeddings.embed_query(text)

            try:
                # [æ€è·¯A] æ‰©å±•å®ä½“ç±»å‹ + é™ä½é˜ˆå€¼
                labels = [
                    "Person", "Organization", "Location", "Event", "Product", "Concept",
                    "Work", "Facility", "Date", "Award", "Technology", "Sport", "Animal"
                ]
                ents = self.entity_model.predict_entities(text, labels, threshold=0.2)  # é™ä½é˜ˆå€¼
                # [æ€è·¯B] å®ä½“å½’ä¸€åŒ–
                unique_ents = {self._normalize_entity(e["text"]): e["label"] for e in ents if self._normalize_entity(e["text"])}
            except Exception:
                unique_ents = {}
            
            # ä½¿ç”¨ REBEL æŠ½å–å…³ç³»ä¸‰å…ƒç»„
            rebel_rels = self._extract_relations_rebel(text)
            
            # æ•´ç†å®ä½“åˆ—è¡¨
            gliner_ents = [{"name": k, "type": v} for k, v in unique_ents.items()]

            all_chunks.append({
                "doc_title": title, "chunk_id": cid, "text": text, "embedding": embedding,
                "entities": gliner_ents,
                "rebel_rels": rebel_rels,  # REBEL æŠ½å–çš„å…³ç³»ä¸‰å…ƒç»„
                "prev_id": f"chunk_{i-1}" if i > 0 else None,
            })

            ids.append(cid)
            lc_docs.append(Document(page_content=text, metadata={"doc_id": cid, "title": title, "type": "chunk"}))

        # Step 2: æ„å»ºæ‘˜è¦æ ‘ (Build Summary Tree)
        docs_map: Dict[str, List[Dict]] = {}
        for c in all_chunks:
            docs_map.setdefault(c["doc_title"], []).append(c)

        summary_nodes_batch = []
        summary_rels_batch = []
        
        for title, chunks in docs_map.items():
            if not chunks: continue

            # --- Level 1 Summaries ---
            vecs = np.array([c["embedding"] for c in chunks])
            n_clusters = max(1, min(len(chunks) // 3, 5)) # Control cluster count
            try:
                kmeans = KMeans(n_clusters=n_clusters, n_init="auto").fit(vecs)
                labels = kmeans.labels_
            except Exception:
                labels = [0] * len(chunks)

            l1_summaries = []
            for c_idx in range(n_clusters):
                cluster_chunks = [chunks[j] for j, lbl in enumerate(labels) if lbl == c_idx]
                if not cluster_chunks: continue
                
                cluster_text = " ".join([c["text"] for c in cluster_chunks])
                summary_text = self._summarize_text(cluster_text, hint=title)
                summary_id = f"summary_l1_{title}_{c_idx}"
                
                l1_summaries.append({"id": summary_id, "text": summary_text, "level": 1, "title": title})
                summary_nodes_batch.append({"id": summary_id, "text": summary_text, "level": 1, "doc_title": title})

                for chunk in cluster_chunks:
                    summary_rels_batch.append({"source": summary_id, "target": chunk["chunk_id"], "type": "CONTAINS_CHUNK"})
                
                ids.append(summary_id)
                lc_docs.append(Document(page_content=summary_text, metadata={"doc_id": summary_id, "title": title, "type": "summary"}))

            # --- Level 2 Summary (Root) ---
            if len(l1_summaries) > 1:
                l1_summary_text = "\n".join([s["text"] for s in l1_summaries])
                l2_summary_text = self._summarize_text(l1_summary_text, hint=f"Overall summary for {title}")
                l2_summary_id = f"summary_l2_{title}"

                summary_nodes_batch.append({"id": l2_summary_id, "text": l2_summary_text, "level": 2, "doc_title": title})
                for l1_node in l1_summaries:
                    summary_rels_batch.append({"source": l2_summary_id, "target": l1_node["id"], "type": "CONTAINS_SUMMARY"})

                ids.append(l2_summary_id)
                lc_docs.append(Document(page_content=l2_summary_text, metadata={"doc_id": l2_summary_id, "title": title, "type": "summary"}))

        # Step3: å†™å…¥å‘é‡åº“ï¼ˆå†…å­˜ï¼‰
        self.vector_store.add_documents(lc_docs, ids=ids)
        try:
            cnt = self.vector_store._collection.count()
            print(f"[DEBUG] Chroma count: {cnt}")
        except Exception as dbg:
            print(f"[DEBUG] Chroma count/get å¤±è´¥: {dbg}")

        # Step4: è¯­ä¹‰è¾¹ RELATED
        semantic_rels = []
        if len(all_chunks) > 1:
            mat = np.array([c["embedding"] for c in all_chunks])
            sim_mat = cosine_similarity(mat)
            rows, cols = np.where(sim_mat > 0.7)
            for r, c in zip(rows, cols):
                if r < c:
                    semantic_rels.append(
                        {
                            "source": all_chunks[r]["chunk_id"],
                            "target": all_chunks[c]["chunk_id"],
                            "score": float(sim_mat[r, c]),
                        }
                    )

        # Step5: å†™ Neo4j
        try:
            with self.driver.session() as session:
                # å†™å…¥ Chunks å’Œ Entities
                session.run(
                    """
                    UNWIND $batch AS row
                    MERGE (d:Document {title: row.doc_title})
                    MERGE (c:Chunk {id: row.chunk_id}) SET c.text = row.text
                    MERGE (d)-[:CONTAINS]->(c)
                    
                    FOREACH (_ IN CASE WHEN row.prev_id IS NOT NULL THEN [1] ELSE [] END |
                        MERGE (p:Chunk {id: row.prev_id}) MERGE (p)-[:NEXT]->(c))
                    
                    FOREACH (e IN row.entities |
                        MERGE (ent:Entity {name: e.name}) SET ent.type = e.type
                        MERGE (c)-[:MENTIONS]->(ent))
                    """,
                    batch=all_chunks,
                )
                
                # å†™å…¥ REBEL æŠ½å–çš„å…³ç³»ä¸‰å…ƒç»„
                session.run(
                    """
                    UNWIND $batch AS row
                    UNWIND row.rebel_rels AS rel
                    MERGE (source:Entity {name: rel.source})
                    MERGE (target:Entity {name: rel.target})
                    MERGE (source)-[r:RELATION {type: rel.type, source_chunk: row.chunk_id}]->(target)
                    """,
                    batch=all_chunks
                )

                # å†™å…¥ Summaries å’Œå…³ç³»
                if summary_nodes_batch:
                    session.run(
                        """
                        UNWIND $batch AS row
                        MERGE (s:Summary {id: row.id}) SET s.text = row.text, s.level = row.level
                        MERGE (d:Document {title: row.doc_title}) MERGE (d)-[:HAS_SUMMARY]->(s)
                        """,
                        batch=summary_nodes_batch,
                    )
                    
                    # Split relationships to avoid MATCH inside FOREACH
                    rels_to_summary = [r for r in summary_rels_batch if r["type"] == "CONTAINS_SUMMARY"]
                    rels_to_chunk = [r for r in summary_rels_batch if r["type"] == "CONTAINS_CHUNK"]

                    if rels_to_summary:
                        session.run(
                            """
                            UNWIND $batch AS row
                            MATCH (source:Summary {id: row.source}), (target:Summary {id: row.target})
                            MERGE (source)-[:CONTAINS]->(target)
                            """,
                            batch=rels_to_summary
                        )

                    if rels_to_chunk:
                        session.run(
                            """
                            UNWIND $batch AS row
                            MATCH (source:Summary {id: row.source}), (target:Chunk {id: row.target})
                            MERGE (source)-[:CONTAINS]->(target)
                            """,
                            batch=rels_to_chunk
                        )

                # å†™å…¥è¯­ä¹‰è¾¹
                if semantic_rels:
                    session.run(
                        """
                        UNWIND $batch AS row
                        MATCH (a:Chunk {id: row.source}), (b:Chunk {id: row.target})
                        MERGE (a)-[r:RELATED]-(b) SET r.score = row.score
                        """,
                        batch=semantic_rels,
                    )
        except Exception as e:
            print(f"âŒ Neo4j Error: {e}")

    def _extract_query_entities(self, query: str) -> List[str]:
        """
        [æ–¹æ¡ˆA + æ€è·¯B] ä»ç”¨æˆ·é—®é¢˜ä¸­æå–å…³é”®å®ä½“ï¼Œç”¨äºå¼•å¯¼å¤šè·³æ£€ç´¢ã€‚
        ä½¿ç”¨æ‰©å±•æ ‡ç­¾ + å½’ä¸€åŒ–å¤„ç†
        """
        try:
            # æ‰©å±•æ ‡ç­¾ç±»å‹
            labels = [
                "Person", "Organization", "Location", "Event", "Product", "Concept",
                "Work", "Facility", "Date", "Award", "Technology", "Sport", "Animal"
            ]
            ents = self.entity_model.predict_entities(query, labels, threshold=0.2)
            # å½’ä¸€åŒ– + å»é‡
            entity_names = list({self._normalize_entity(e["text"]) for e in ents if self._normalize_entity(e["text"])})
            if entity_names:
                print(f"ğŸ¯ Query Entities (normalized): {entity_names}")
            return entity_names
        except Exception as e:
            print(f"âš ï¸ Query Entity Extraction Error: {e}")
            return []

    def _compute_trust_score(self, node: Dict, query_entities: List[str], reranker_score: float, hop_depth: int) -> float:
        """
        [æ–¹æ¡ˆC] å¤šä¿¡å·å¯ä¿¡åº¦è¯„åˆ† (Multi-Signal Trustworthiness)
        èåˆå¤šä¸ªä¿¡å·è®¡ç®—èŠ‚ç‚¹çš„å¯ä¿¡åº¦ï¼š
        1. Reranker è¯­ä¹‰ç›¸å…³æ€§ (55%) - ä¸»è¦ä¿¡å·ï¼Œæƒé‡æé«˜
        2. å®ä½“è¦†ç›–ç‡ (15%) - è¾…åŠ©ä¿¡å·ï¼Œæƒé‡é™ä½é¿å…è¿‡åº¦æƒ©ç½š
        3. è·¯å¾„é•¿åº¦æƒ©ç½š (15%) - è¶ŠçŸ­è¶Šå¯ä¿¡
        4. æ¥æºç±»å‹åŠ æƒ (15%) - QueryEnt > EntBridge > Sem > Seq
        """
        # 1. Reranker åˆ†æ•°å½’ä¸€åŒ– (sigmoid å˜æ¢åˆ° 0-1)
        reranker_norm = 1 / (1 + np.exp(-reranker_score / 5))  # é™¤ä»¥5ç¼“å’Œæ›²çº¿
        
        # 2. å®ä½“è¦†ç›–ç‡ (è½¯åŒ–å¤„ç†ï¼šå³ä½¿æ²¡æœ‰å®ä½“åŒ¹é…ä¹Ÿç»™ä¸€ä¸ªåŸºç¡€åˆ†)
        entity_coverage = 0.3  # åŸºç¡€åˆ†ï¼Œé¿å…å®Œå…¨ä¸º0
        if query_entities:
            text_lower = node.get("text", "").lower()
            matched = sum(1 for ent in query_entities if ent in text_lower)
            entity_coverage = 0.3 + 0.7 * (matched / len(query_entities))  # 0.3-1.0 èŒƒå›´
        
        # 3. è·¯å¾„é•¿åº¦æƒ©ç½š (hop_depth è¶Šå¤§æƒ©ç½šè¶Šé‡ï¼Œä½†ç¼“å’Œ)
        path_penalty = 1.0 / (1 + 0.15 * hop_depth)  # hop=0: 1.0, hop=2: 0.77, hop=4: 0.625
        
        # 4. æ¥æºç±»å‹åŠ æƒ
        source_weights = {
            "QueryEnt": 1.0,      # ç›´æ¥å‘½ä¸­é—®é¢˜å®ä½“
            "EntBridge": 0.90,   # å®ä½“å…±ç°æ¡¥æ¥
            "ActionPath": 0.85,  # é€šè¿‡å®ä½“å…³ç³»è·¯å¾„
            "SummaryDrill": 0.85, # æ‘˜è¦ä¸‹é’»
            "Sem": 0.80,         # è¯­ä¹‰ç›¸ä¼¼
            "VectorJump": 0.75,  # å‘é‡è·³è½¬
            "Seq": 0.65,         # é¡ºåºæ‰©å±•
        }
        # ä» title ä¸­æå–æ¥æºç±»å‹ï¼ˆå¦‚æœæ˜¯çœŸå® title åˆ™é»˜è®¤ä¸º EntBridgeï¼‰
        title = node.get("title", "")
        source_type = title if title in source_weights else "EntBridge"
        source_weight = source_weights.get(source_type, 0.75)
        
        # åŠ æƒèåˆ
        trust_score = (
            0.55 * reranker_norm +
            0.15 * entity_coverage +
            0.15 * path_penalty +
            0.15 * source_weight
        )
        
        return trust_score

    def _summary_guided_retrieval(self, user_query: str, query_entities: List[str], top_k: int = 5) -> List[Dict]:
        """
        [æ–¹æ¡ˆE] Summary-Guided Top-Down Retrieval
        ä» Summary Tree é¡¶å±‚å¼€å§‹ï¼Œé€šè¿‡æ‘˜è¦å®šä½ç›¸å…³ä¸»é¢˜ï¼Œç„¶åä¸‹é’»åˆ°å…·ä½“ Chunkã€‚
        è¿™æ ·å¯ä»¥åˆ©ç”¨æ‘˜è¦æ ‘çš„å…¨å±€è§†è§’ï¼Œé¿å…ä¸€å¼€å§‹å°±é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚
        """
        candidates = []
        try:
            # Step 1: æ£€ç´¢æœ€ç›¸å…³çš„ Summary èŠ‚ç‚¹
            summary_results = self.vector_store.similarity_search_with_score(
                user_query, 
                k=top_k,
                filter={"type": "summary"}  # åªæ£€ç´¢æ‘˜è¦èŠ‚ç‚¹
            )
            
            if not summary_results:
                # å¦‚æœæ²¡æœ‰æ‘˜è¦èŠ‚ç‚¹ï¼Œå›é€€åˆ°æ™®é€šæ£€ç´¢
                return []
            
            # Step 2: ä»æ¯ä¸ªç›¸å…³ Summary ä¸‹é’»åˆ° Chunk
            for summary_doc, score in summary_results:
                summary_id = summary_doc.metadata.get("doc_id")
                children = self._get_summary_children(summary_id)
                
                for child in children:
                    # å¦‚æœå­èŠ‚ç‚¹è¿˜æ˜¯ Summaryï¼Œç»§ç»­ä¸‹é’»
                    if child["id"].startswith("summary_"):
                        grandchildren = self._get_summary_children(child["id"])
                        for gc in grandchildren:
                            if not gc["id"].startswith("summary_"):
                                candidates.append({
                                    "id": gc["id"],
                                    "text": gc["text"],
                                    "title": "SummaryDrill",
                                    "source_summary": summary_id
                                })
                    else:
                        candidates.append({
                            "id": child["id"],
                            "text": child["text"],
                            "title": "SummaryDrill",
                            "source_summary": summary_id
                        })
                
                if len(candidates) >= top_k * 2:
                    break
            
            if candidates:
                print(f"  ğŸŒ³ Summary-Guided ä¸‹é’»äº† {len(candidates)} ä¸ª Chunk")
                
        except Exception as e:
            # filter å¯èƒ½ä¸è¢«æ”¯æŒï¼Œå›é€€åˆ°æ™®é€šæ–¹å¼
            print(f"âš ï¸ Summary-Guided Retrieval fallback: {e}")
        
        return candidates[:top_k]

    def query_adaptive_search(self, user_query: str, beam_width: int = 3, max_hops: int = 3) -> str:
        """
        è‡ªé€‚åº”å¤šè·³æ£€ç´¢ (Adaptive Multi-hop Search) - Best-First Search ç‰ˆ
        æ ¸å¿ƒåˆ›æ–°ç‚¹:
        1. Global Best-First Strategy: ç»´æŠ¤å…¨å±€å€™é€‰æ± ï¼Œè‡ªç„¶æ”¯æŒå›æº¯ (Backtracking)ã€‚
        2. Context-Aware Reranking: å§‹ç»ˆæºå¸¦è·¯å¾„ä¸Šä¸‹æ–‡ã€‚
        3. [æ–¹æ¡ˆC] Multi-Signal Trustworthiness: å¤šä¿¡å·å¯ä¿¡åº¦è¯„åˆ†ã€‚
        4. [æ–¹æ¡ˆA] Query-Guided Entity Linking: ä»é—®é¢˜æå–å®ä½“ï¼Œå¼•å¯¼å¤šè·³æ‰©å±•ã€‚
        5. [æ–¹æ¡ˆD] Entity Relation Path: é€šè¿‡ ACTION è¾¹æ‰©å±•ã€‚
        6. [æ–¹æ¡ˆE] Summary-Guided Top-Down: æ‘˜è¦æ ‘å¼•å¯¼æ£€ç´¢ã€‚
        """
        print(f"ğŸ” Starting Adaptive Search (Max Hops: {max_hops}, Beam: {beam_width})")
        
        # [æ–¹æ¡ˆA] æå–é—®é¢˜ä¸­çš„å®ä½“ï¼Œç”¨äºå¼•å¯¼åç»­æ‰©å±•
        query_entities = self._extract_query_entities(user_query)
        
        # --- 1. åˆå§‹åŒ–ç§å­èŠ‚ç‚¹ ---
        initial_candidates = []
        
        # [æ–¹æ¡ˆE] Summary-Guided Top-Down Retrieval: å…ˆä»æ‘˜è¦æ ‘ä¸‹é’»
        summary_candidates = self._summary_guided_retrieval(user_query, query_entities, top_k=beam_width)
        for sc in summary_candidates:
            initial_candidates.append({
                "id": sc["id"],
                "text": sc["text"],
                "title": sc.get("title", "SummaryDrill"),
                "path_history": [],
                "context_str": "",
                "hop_depth": 0
            })
        
        # æ™®é€šå‘é‡æ£€ç´¢è¡¥å……
        try:
            seed_docs = self.vector_store.similarity_search_with_score(user_query, k=beam_width * 2)
            
            for doc, _ in seed_docs:
                d_id = doc.metadata.get("doc_id")
                d_type = doc.metadata.get("type")
                
                # è·³è¿‡å·²æœ‰çš„å€™é€‰
                if any(c["id"] == d_id for c in initial_candidates):
                    continue
                
                # æ‘˜è¦èŠ‚ç‚¹å±•å¼€
                if d_type == "summary":
                    children = self._get_summary_children(d_id)
                    for child in children:
                        if not any(c["id"] == child["id"] for c in initial_candidates):
                            initial_candidates.append({
                                "id": child["id"], 
                                "text": child["text"], 
                                "title": child["title"],
                                "path_history": [],
                                "context_str": "",
                                "hop_depth": 0
                            })
                else:
                    initial_candidates.append({
                        "id": d_id, 
                        "text": doc.page_content, 
                        "title": doc.metadata.get("title", ""),
                        "path_history": [],
                        "context_str": "",
                        "hop_depth": 0
                    })
        except Exception as e:
            print(f"âŒ Init Search Error: {e}")
            if not initial_candidates:
                return "I don't know."

        if not initial_candidates: return "I don't know."

        # --- 2. å…¨å±€ä¼˜å…ˆé˜Ÿåˆ— (Global Frontier) ---
        # ç»“æ„: List[Dict]
        # æˆ‘ä»¬åœ¨æ¯ä¸€æ­¥éƒ½ Rerank æ•´ä¸ª Frontier (æˆ–è€… Frontier çš„ Top N)ï¼Œç„¶åé€‰æœ€å¥½çš„æ‰©å±•
        frontier = initial_candidates
        visited_ids = set()
        final_selected_nodes = {} # id -> node_data (å»é‡åçš„æœ€ç»ˆè¯æ®)
        
        # åˆå§‹æ‰“åˆ† [æ–¹æ¡ˆC] ä½¿ç”¨å¤šä¿¡å·å¯ä¿¡åº¦è¯„åˆ†
        pairs = [[user_query, c["text"]] for c in frontier]
        reranker_scores = self.reranker.compute_score(pairs)
        if isinstance(reranker_scores, float): reranker_scores = [reranker_scores]
        
        for i, node in enumerate(frontier):
            # [æ–¹æ¡ˆC] å¤šä¿¡å·èåˆè¯„åˆ†
            node["reranker_score"] = reranker_scores[i]
            node["trust_score"] = self._compute_trust_score(
                node, query_entities, reranker_scores[i], node.get("hop_depth", 0)
            )
            node["score"] = node["trust_score"]  # ä½¿ç”¨å¯ä¿¡åº¦ä½œä¸ºæ’åºä¾æ®
            # åˆå§‹è·¯å¾„å°±æ˜¯å®ƒè‡ªå·±
            node["path_history"] = [f"Start -> '{node['title']}'"]
            node["context_str"] = f"[{node['title']}] {node['text']}"

        # æŒ‰åˆ†æ•°æ’åº
        frontier.sort(key=lambda x: x["score"], reverse=True)
        frontier = frontier[:beam_width] # åªä¿ç•™åˆå§‹æœ€å¥½çš„å‡ ä¸ª

        # --- 3. è¿­ä»£æ‰©å±• (Best-First Loop) ---
        step = 0
        while step < max_hops and frontier:
            step += 1
            print(f"--- Step {step} (Frontier Size: {len(frontier)}) ---")
            
            # å–å‡ºå½“å‰æœ€å¥½çš„èŠ‚ç‚¹è¿›è¡Œæ‰©å±• (Pop Best)
            # æ³¨æ„ï¼šä¸ºäº†æ¨¡æ‹Ÿ Beam Search çš„å®½åº¦ï¼Œæˆ‘ä»¬è¿™é‡Œå¯ä»¥ä¸€æ¬¡å– Top 1 æˆ– Top K æ‰©å±•
            # è¿™é‡Œé‡‡ç”¨ï¼šå– Top 1 è¿›è¡Œæ‰©å±•ï¼Œç„¶åå°†æ–°èŠ‚ç‚¹åŠ å…¥ Frontier å†æ’åº
            # è¿™æ ·èƒ½æœ€å¤§ç¨‹åº¦ä½“ç° "Backtracking"ï¼šå¦‚æœæ–°æ‰©å±•çš„èŠ‚ç‚¹åˆ†æ•°çƒ‚ï¼Œä¸‹æ¬¡å¾ªç¯å°±ä¼šå–åŸæ¥ç¬¬äºŒå¥½çš„
            
            current_best_node = frontier.pop(0) # å–å‡ºç¬¬ä¸€å
            
            if current_best_node["id"] in visited_ids:
                continue
            
            visited_ids.add(current_best_node["id"])
            
            # [æ–¹æ¡ˆC] ä½¿ç”¨å¯ä¿¡åº¦é˜ˆå€¼è¿›è¡Œå‰ªæ
            # å¯ä¿¡åº¦èŒƒå›´ 0-1ï¼Œé˜ˆå€¼é™ä½åˆ° 0.2 é¿å…è¿‡åº¦å‰ªæ
            trust_threshold = 0.2
            if current_best_node["score"] >= trust_threshold:
                final_selected_nodes[current_best_node["id"]] = current_best_node
                print(f"  âœ… Selected: {current_best_node['title']} (Trust: {current_best_node['score']:.3f}, Reranker: {current_best_node.get('reranker_score', 0):.2f})")
            else:
                print(f"  ğŸ—‘ï¸ Pruned: {current_best_node['title']} (Low Trust: {current_best_node['score']:.3f})")
                continue # å¯ä¿¡åº¦å¤ªä½ï¼Œä¸æ‰©å±•è¿™æ¡è·¯äº† (Pruning)

            # æ‰©å±•é‚»å±… [æ–¹æ¡ˆA] ä¼ å…¥ query_entities è¿›è¡Œå®šå‘æ‰©å±•
            neighbors_map = self._expand_node(current_best_node["id"], visited_ids, query_entities)
            
            # [æ–¹æ¡ˆB] Hybrid Retrieval: å¦‚æœå›¾æ‰©å±•ç»“æœä¸è¶³ï¼Œç”¨ç´¯ç§¯ä¸Šä¸‹æ–‡åšå‘é‡æ£€ç´¢è¡¥å……
            if len(neighbors_map) < 3:
                hybrid_candidates = self._hybrid_vector_retrieval(
                    user_query, 
                    current_best_node["context_str"], 
                    visited_ids,
                    top_k=5
                )
                for hc in hybrid_candidates:
                    if hc["id"] not in neighbors_map:
                        neighbors_map[hc["id"]] = {"text": hc["text"], "title": hc["title"]}
            
            if not neighbors_map:
                continue

            # å‡†å¤‡æ–°çš„ä¸€æ‰¹å€™é€‰é¡¹
            new_candidates = []
            rerank_pairs = []
            
            # æ„é€  Context-Aware Query
            # ä½¿ç”¨å½“å‰èŠ‚ç‚¹ç´¯ç§¯çš„ä¸Šä¸‹æ–‡
            current_context = current_best_node["context_str"][-1000:].replace("\n", " ")
            rerank_query = f"{user_query} [Context: {current_context}]"

            # --- [Scheme B] Hybrid Retrieval: å‘é‡å…œåº•æ‰©å±• ---
            # åˆ©ç”¨æœ‰äº†ä¸Šä¸‹æ–‡çš„æ–° Queryï¼Œå»å…¨å±€å‘é‡åº“é‡Œå†æä¸€æŠŠï¼Œè·³å‡ºå±€éƒ¨å›¾é™åˆ¶
            try:
                # æ£€ç´¢ Top-K (æ•°é‡ä¸ beam_width ç›¸å½“å³å¯)
                vector_candidates = self.vector_store.similarity_search_with_score(rerank_query, k=beam_width)
                
                for v_doc, v_score in vector_candidates:
                    v_id = v_doc.metadata.get("doc_id")
                    if v_id in visited_ids: continue # é¿å…å›å¤´è·¯
                    
                    # æ ¼å¼åŒ–ä¸ºæ ‡å‡†èŠ‚ç‚¹
                    v_node = {
                        "id": v_id,
                        "text": v_doc.page_content,
                        "title": "VectorJump",  # æ ‡è®°ä¸ºå‘é‡è·³è½¬ç±»å‹
                        # è®°å½•è¿™æ˜¯ä¸€ä¸ªè·³è·ƒæ­¥éª¤
                        "path_history": current_best_node["path_history"] + [f"-> [VectorJump] '{v_doc.metadata.get('title', '')}'"],
                        "context_str": current_best_node["context_str"] + f"\n[{v_doc.metadata.get('title', '')}] {v_doc.page_content}",
                        "hop_depth": current_best_node.get("hop_depth", 0) + 1,
                        "source_type": "VectorJump"
                    }
                    new_candidates.append(v_node)
            except Exception as e:
                print(f"âš ï¸ Vector Expansion Error: {e}")

            # --- Graph Expansion ---
            current_hop = current_best_node.get("hop_depth", 0) + 1
            for n_id, n_data in neighbors_map.items():
                if n_id in visited_ids: continue
                
                new_node = {
                    "id": n_id,
                    "text": n_data["text"],
                    "title": n_data["title"],
                    # ç»§æ‰¿è·¯å¾„å’Œä¸Šä¸‹æ–‡
                    "path_history": current_best_node["path_history"] + [f"-> '{n_data['title']}'"],
                    "context_str": current_best_node["context_str"] + f"\n[{n_data['title']}] {n_data['text']}",
                    "hop_depth": current_hop,
                    "source_type": n_data["title"]  # è®°å½•æ¥æºç±»å‹ç”¨äºå¯ä¿¡åº¦è®¡ç®—
                }
                new_candidates.append(new_node)

            # ç»Ÿä¸€æ„é€  rerank_pairs
            rerank_pairs = []
            for node in new_candidates:
                rerank_pairs.append([rerank_query, node["text"]])

            if not new_candidates: continue

            # æ‰¹é‡æ‰“åˆ† [æ–¹æ¡ˆC] ä½¿ç”¨å¤šä¿¡å·å¯ä¿¡åº¦è¯„åˆ†
            reranker_scores = self.reranker.compute_score(rerank_pairs)
            if isinstance(reranker_scores, float): reranker_scores = [reranker_scores]

            for i, node in enumerate(new_candidates):
                node["reranker_score"] = reranker_scores[i]
                node["trust_score"] = self._compute_trust_score(
                    node, query_entities, reranker_scores[i], node.get("hop_depth", 0)
                )
                node["score"] = node["trust_score"]  # ä½¿ç”¨å¯ä¿¡åº¦ä½œä¸ºæ’åºä¾æ®
            
            # å°†æ–°èŠ‚ç‚¹åŠ å…¥ Frontier
            frontier.extend(new_candidates)
            
            # é‡æ–°æ’åº Frontier
            frontier.sort(key=lambda x: x["score"], reverse=True)
            
            # ä¿æŒ Frontier å¤§å°é€‚ä¸­ï¼Œé˜²æ­¢çˆ†ç‚¸
            frontier = frontier[:beam_width * 2]

        # --- 4. ç”Ÿæˆç­”æ¡ˆ ---
        if not final_selected_nodes:
            return "I don't know."

        # æ•´ç†æœ€ç»ˆä¸Šä¸‹æ–‡å’Œæ¨ç†è·¯å¾„
        # æŒ‰ç…§è¢«é€‰ä¸­çš„é¡ºåºï¼ˆåˆ†æ•°é«˜ä½ï¼‰æ’åˆ—
        sorted_evidence = sorted(final_selected_nodes.values(), key=lambda x: x["score"], reverse=True)
        
        context_str = "\n\n".join([f"[{n['title']}] {n['text']}" for n in sorted_evidence])
        # è¿™é‡Œçš„ Path å¯èƒ½æœ‰å¤šæ¡ï¼Œæˆ‘ä»¬å±•ç¤ºå¾—åˆ†æœ€é«˜çš„é‚£æ¡è·¯å¾„
        best_path_str = " -> ".join(sorted_evidence[0]["path_history"])

        prompt = f"""You are a precise QA system. Answer the question based on the provided context.

**Rules:**
1. Answer strictly with the Entity Name, Date, Location, or Phrase.
2. Be extremely concise. Avoid full sentences.
3. For Yes/No questions, output ONLY 'yes' or 'no'.
4. If the answer allows for a reasonable inference from the context, provide it. Only return 'I don't know' if the context is completely irrelevant.

**Reasoning Path:**
{best_path_str}

**Context:**
{context_str}

Question: {user_query}
Answer:"""
        return self.llm.invoke(prompt).content

    def _hybrid_vector_retrieval(self, user_query: str, context_str: str, visited_ids: Set[str], top_k: int = 5) -> List[Dict]:
        """
        [æ–¹æ¡ˆB] Hybrid Retrieval: ç”¨ç´¯ç§¯ä¸Šä¸‹æ–‡æ„é€ å¢å¼ºæŸ¥è¯¢ï¼Œå›åˆ°å‘é‡åº“æ£€ç´¢è¡¥å……å€™é€‰ã€‚
        å½“å›¾æ‰©å±•å¤±è´¥æ—¶ï¼Œè¿™ä¸ªæ–¹æ³•å¯ä»¥å…œåº•ã€‚
        """
        candidates = []
        try:
            # æ„é€ å¢å¼ºæŸ¥è¯¢ï¼šåŸé—®é¢˜ + å½“å‰ä¸Šä¸‹æ–‡çš„å…³é”®ä¿¡æ¯
            # æˆªå–ä¸Šä¸‹æ–‡çš„å‰500å­—ç¬¦ä½œä¸ºè¡¥å……ä¿¡æ¯
            context_snippet = context_str[:500].replace("\n", " ").strip()
            enhanced_query = f"{user_query} {context_snippet}"
            
            # å‘é‡æ£€ç´¢
            results = self.vector_store.similarity_search_with_score(enhanced_query, k=top_k * 2)
            
            for doc, score in results:
                d_id = doc.metadata.get("doc_id")
                d_type = doc.metadata.get("type")
                
                # è·³è¿‡å·²è®¿é—®çš„å’Œæ‘˜è¦èŠ‚ç‚¹
                if d_id in visited_ids or d_type == "summary":
                    continue
                
                candidates.append({
                    "id": d_id,
                    "text": doc.page_content,
                    "title": doc.metadata.get("title", "VecRetrieval")
                })
                
                if len(candidates) >= top_k:
                    break
            
            if candidates:
                print(f"  ğŸ”„ Hybrid Retrievalè¡¥å……äº† {len(candidates)} ä¸ªå€™é€‰")
                
        except Exception as e:
            print(f"âš ï¸ Hybrid Retrieval Error: {e}")
        
        return candidates

    def _get_summary_children(self, summary_id: str) -> List[Dict]:
        """è·å–ä¸€ä¸ªæ‘˜è¦èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ï¼ˆä¸‹ä¸€å±‚æ‘˜è¦æˆ–æ–‡æœ¬å—ï¼‰"""
        children = []
        with self.driver.session() as s:
            # æŸ¥è¯¢æ‰€æœ‰è¢«è¯¥æ‘˜è¦èŠ‚ç‚¹ CONTAINS çš„ Summary æˆ– Chunk
            res = s.run(
                """
                MATCH (parent:Summary {id: $id})-[:CONTAINS]->(child)
                RETURN child.id AS id, child.text AS text, labels(child)[0] AS type
                """,
                id=summary_id,
            )
            for r in res:
                node_type = r["type"]
                title = "Summary" if node_type == "Summary" else self.doc_cache.get(r["id"], {}).get("title", "")
                children.append({"id": r["id"], "text": r["text"], "title": title})
        return children

    def _expand_node(self, c_id: str, visited: Set[str], query_entities: List[str] = None) -> Dict[str, Dict]:
        """
        æ‰©å±•èŠ‚ç‚¹çš„é‚»å±…ã€‚
        [æ–¹æ¡ˆA] æ–°å¢ query_entities å‚æ•°ï¼Œä¼˜å…ˆæ£€ç´¢åŒ…å«é—®é¢˜å®ä½“çš„ Chunkã€‚
        [æ–¹æ¡ˆD] é€šè¿‡ ACTION è¾¹ï¼ˆå®ä½“å…³ç³»è·¯å¾„ï¼‰æ‰©å±•ã€‚
        """
        data: Dict[str, Dict] = {}
        query_entities = query_entities or []
        
        # åŸºç¡€æ‰©å±•æŸ¥è¯¢ (Sequential + Semantic + Co-occurrence + [Scheme D] Action Path)
        base_query = """
        MATCH (s:Chunk {id: $id})
        // 1. Sequential expansion
        OPTIONAL MATCH (s)-[:NEXT]-(n:Chunk) WHERE NOT n.id IN $vis
        // 2. Semantic similarity expansion
        OPTIONAL MATCH (s)-[r:RELATED]-(sem:Chunk) WHERE r.score > 0.7 AND NOT sem.id IN $vis
        // 3. Entity bridge expansion (co-occurrence)
        OPTIONAL MATCH (s)-[:MENTIONS]->(:Entity)<-[:MENTIONS]-(b:Chunk) WHERE NOT b.id IN $vis
        // 4. [Scheme D] Spacy Action Path expansion
        OPTIONAL MATCH (s)-[:MENTIONS]->(:Entity)-[:ACTION]->(:Entity)<-[:MENTIONS]-(act:Chunk) WHERE NOT act.id IN $vis
        
        RETURN n.id, n.text, sem.id, sem.text, b.id, b.text, act.id, act.text LIMIT 20
        """
        with self.driver.session() as s:
            res = s.run(base_query, id=c_id, vis=list(visited))
            for r in res:
                if r["n.id"]:
                    data[r["n.id"]] = {"text": r["n.text"], "title": "Seq"}
                if r["sem.id"]:
                    data[r["sem.id"]] = {"text": r["sem.text"], "title": "Sem"}
                if r["b.id"]:
                    data[r["b.id"]] = {"text": r["b.text"], "title": "EntBridge"}
                if r["act.id"]:
                    data[r["act.id"]] = {"text": r["act.text"], "title": "ActionPath"}
        
        # [æ–¹æ¡ˆA + æ€è·¯E] Query-Guided Entity Linking with Fuzzy Matching
        # ä½¿ç”¨ CONTAINS æ¨¡ç³ŠåŒ¹é…ï¼Œè§£å†³å®ä½“åä¸å®Œå…¨ä¸€è‡´çš„é—®é¢˜
        if query_entities:
            for qe in query_entities:
                # å½’ä¸€åŒ–æŸ¥è¯¢å®ä½“
                qe_norm = self._normalize_entity(qe)
                if not qe_norm:
                    continue
                    
                # æ¨¡ç³ŠåŒ¹é…ï¼šå®ä½“ååŒ…å«æŸ¥è¯¢è¯ æˆ– æŸ¥è¯¢è¯åŒ…å«å®ä½“å
                entity_query = """
                MATCH (ent:Entity)<-[:MENTIONS]-(c:Chunk)
                WHERE (ent.name CONTAINS $entity OR $entity CONTAINS ent.name) 
                      AND NOT c.id IN $vis
                RETURN DISTINCT c.id AS id, c.text AS text LIMIT 5
                """
                try:
                    with self.driver.session() as s:
                        res = s.run(entity_query, entity=qe_norm, vis=list(visited))
                        for r in res:
                            if r["id"] and r["id"] not in data:
                                data[r["id"]] = {"text": r["text"], "title": "QueryEnt"}
                except Exception:
                    pass

        # è¡¥å…… title ä¿¡æ¯
        for k in data:
            if k in self.doc_cache:
                data[k]["title"] = self.doc_cache[k].get("title", data[k]["title"])
        return data

    def close(self):
        if self.driver:
            self.driver.close()
