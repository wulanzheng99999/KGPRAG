"""
æ£€ç´¢æ¨¡å—ï¼šå¤šè·³æ£€ç´¢ä¸å¯ä¿¡åº¦è¯„åˆ†
"""
from __future__ import annotations

from typing import List, Dict, Set, Tuple

import numpy as np
import networkx as nx
from FlagEmbedding import FlagReranker

from src.config import (
    DEVICE, RERANKER_MODEL, DEFAULT_BEAM_WIDTH, DEFAULT_MAX_HOPS, 
    TRUST_THRESHOLD, SMALL_SPACE_THRESHOLD, MIN_CANDIDATES_KEEP
)
from src.entity_extractor import EntityExtractor
from src.graph_store import GraphStore
from src.vector_store import VectorStore


class MultiHopRetriever:
    """å¤šè·³æ£€ç´¢å™¨"""
    
    def __init__(self, entity_extractor: EntityExtractor, graph_store: GraphStore, vector_store: VectorStore):
        self.entity_extractor = entity_extractor
        self.graph_store = graph_store
        self.vector_store = vector_store
        
        print(f"ğŸ“¦ åŠ è½½é‡æ’æ¨¡å‹: {RERANKER_MODEL}")
        self.reranker = FlagReranker(RERANKER_MODEL, use_fp16=(DEVICE == "cuda"))
        
        # Reranker ç¼“å­˜ (é¿å…é‡å¤è®¡ç®—)
        self._reranker_cache: Dict[str, float] = {}
        
        # PPR åˆ†æ•°ç¼“å­˜ (æ¯ä¸ªé—®é¢˜é‡ç½®)
        self._ppr_scores: Dict[str, float] = {}
    
    def _get_reranker_scores(self, pairs: List[List[str]]) -> List[float]:
        """å¸¦ç¼“å­˜çš„ Reranker è°ƒç”¨"""
        results = []
        uncached_pairs = []
        uncached_indices = []
        
        for i, pair in enumerate(pairs):
            cache_key = f"{pair[0][:100]}|||{pair[1][:100]}"  # æˆªæ–­é¿å… key è¿‡é•¿
            if cache_key in self._reranker_cache:
                results.append(self._reranker_cache[cache_key])
            else:
                results.append(None)  # å ä½
                uncached_pairs.append(pair)
                uncached_indices.append(i)
        
        # æ‰¹é‡è®¡ç®—æœªç¼“å­˜çš„
        if uncached_pairs:
            scores = self.reranker.compute_score(uncached_pairs)
            if isinstance(scores, float):
                scores = [scores]
            
            for idx, score in zip(uncached_indices, scores):
                results[idx] = score
                cache_key = f"{pairs[idx][0][:100]}|||{pairs[idx][1][:100]}"
                self._reranker_cache[cache_key] = score
        
        return results
    
    def reset_cache(self):
        """é‡ç½®ç¼“å­˜ï¼ˆæ¯ä¸ªé—®é¢˜å¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
        self._reranker_cache = {}
        self._ppr_scores = {}
    
    def _build_graph_and_compute_ppr(self, query_entities: List[str], alpha: float = 0.85) -> Dict[str, float]:
        """
        æ„å»ºå†…å­˜å›¾å¹¶è®¡ç®— Personalized PageRank
        
        å‚æ•°:
            query_entities: æŸ¥è¯¢ä¸­çš„å®ä½“åˆ—è¡¨ï¼ˆä½œä¸ºç§å­èŠ‚ç‚¹ï¼‰
            alpha: é˜»å°¼ç³»æ•° (0.85 æ˜¯ç»å…¸å€¼)
        
        è¿”å›:
            {chunk_id: ppr_score, ...}
        """
        # å¦‚æœæ²¡æœ‰æŸ¥è¯¢å®ä½“ï¼Œè·³è¿‡ PPR
        if not query_entities:
            print("  â­ï¸ PPR skipped: no query entities")
            return {}
        
        G = nx.Graph()
        
        try:
            with self.graph_store.driver.session() as session:
                # 1. è·å–æ‰€æœ‰ Chunk èŠ‚ç‚¹
                result = session.run("MATCH (c:Chunk) RETURN c.id AS id")
                for record in result:
                    G.add_node(record["id"])
                
                if len(G.nodes) == 0:
                    print("  â­ï¸ PPR skipped: no chunks in graph")
                    return {}
                
                # 2. è·å–æ‰€æœ‰è¾¹ (åˆ†å¼€æŸ¥è¯¢ï¼Œé¿å… UNION è¶…æ—¶)
                # NEXT è¾¹
                result = session.run("""
                    MATCH (c1:Chunk)-[:NEXT]-(c2:Chunk)
                    RETURN DISTINCT c1.id AS source, c2.id AS target
                """)
                for record in result:
                    if record["source"] in G.nodes and record["target"] in G.nodes:
                        G.add_edge(record["source"], record["target"])
                
                # RELATED è¾¹
                result = session.run("""
                    MATCH (c1:Chunk)-[:RELATED]-(c2:Chunk)
                    RETURN DISTINCT c1.id AS source, c2.id AS target
                """)
                for record in result:
                    if record["source"] in G.nodes and record["target"] in G.nodes:
                        G.add_edge(record["source"], record["target"])
                
                # Entity Bridge è¾¹ (é™åˆ¶æ•°é‡é¿å…è¶…æ—¶)
                result = session.run("""
                    MATCH (c1:Chunk)-[:MENTIONS]->(:Entity)<-[:MENTIONS]-(c2:Chunk)
                    WHERE c1.id < c2.id
                    RETURN DISTINCT c1.id AS source, c2.id AS target
                    LIMIT 500
                """)
                for record in result:
                    if record["source"] in G.nodes and record["target"] in G.nodes:
                        G.add_edge(record["source"], record["target"])
                
                # 3. æ„å»º personalization å‘é‡
                personalization = {}
                for entity in query_entities[:5]:  # é™åˆ¶æœ€å¤š 5 ä¸ªå®ä½“
                    result = session.run("""
                        MATCH (e:Entity)<-[:MENTIONS]-(c:Chunk)
                        WHERE e.name CONTAINS $entity OR $entity CONTAINS e.name
                        RETURN c.id AS chunk_id
                        LIMIT 10
                    """, entity=entity)
                    for record in result:
                        chunk_id = record["chunk_id"]
                        if chunk_id in G.nodes:
                            personalization[chunk_id] = personalization.get(chunk_id, 0) + 1.0
                
                # å½’ä¸€åŒ–
                if personalization:
                    total = sum(personalization.values())
                    personalization = {k: v/total for k, v in personalization.items()}
                else:
                    print("  â­ï¸ PPR skipped: no seed nodes found")
                    return {}
                
        except Exception as e:
            print(f"  âš ï¸ PPR Graph Build Error: {e}")
            return {}
        
        # 4. è®¡ç®— PPR
        if len(G.edges) == 0:
            print("  â­ï¸ PPR skipped: no edges in graph")
            return {}
        
        try:
            ppr_scores = nx.pagerank(G, alpha=alpha, personalization=personalization, max_iter=30, tol=1e-4)
            
            # å½’ä¸€åŒ–åˆ° [0, 1]
            if ppr_scores:
                max_score = max(ppr_scores.values())
                min_score = min(ppr_scores.values())
                if max_score > min_score:
                    ppr_scores = {k: (v - min_score) / (max_score - min_score) for k, v in ppr_scores.items()}
            
            print(f"  ğŸ“Š PPR: {len(ppr_scores)} nodes, {len(G.edges)} edges, {len(personalization)} seeds")
            return ppr_scores
            
        except Exception as e:
            print(f"  âš ï¸ PPR Compute Error: {e}")
            return {}
    
    def compute_trust_score(self, node: Dict, query_entities: List[str], reranker_score: float, hop_depth: int) -> float:
        """
        å¤šä¿¡å·å¯ä¿¡åº¦è¯„åˆ† (Multi-Signal Trustworthiness)
        èåˆå¤šä¸ªä¿¡å·è®¡ç®—èŠ‚ç‚¹çš„å¯ä¿¡åº¦ï¼š
        1. Reranker è¯­ä¹‰ç›¸å…³æ€§ (50%)
        2. PPR æ‹“æ‰‘é‡è¦æ€§ (10%) - ä»…å½“ PPR æœ‰æ•ˆæ—¶
        3. å®ä½“è¦†ç›–ç‡ (15%)
        4. è·¯å¾„é•¿åº¦æƒ©ç½š (12%)
        5. æ¥æºç±»å‹åŠ æƒ (13%)
        """
        # 1. Reranker åˆ†æ•°å½’ä¸€åŒ–
        reranker_norm = 1 / (1 + np.exp(-reranker_score / 5))
        
        # 2. PPR åˆ†æ•°ï¼ˆä»…å½“ PPR æœ‰æ•ˆæ—¶ä½¿ç”¨ï¼‰
        node_id = node.get("id", "")
        ppr_available = bool(self._ppr_scores)  # PPR æ˜¯å¦æœ‰æ•ˆ
        ppr_score = self._ppr_scores.get(node_id, 0.0) if ppr_available else 0.0
        
        # 3. å®ä½“è¦†ç›–ç‡
        entity_coverage = 0.3  # åŸºç¡€åˆ†
        if query_entities:
            text_lower = node.get("text", "").lower()
            matched = sum(1 for ent in query_entities if ent in text_lower)
            entity_coverage = 0.3 + 0.7 * (matched / len(query_entities))
        
        # 4. è·¯å¾„é•¿åº¦æƒ©ç½š
        path_penalty = 1.0 / (1 + 0.15 * hop_depth)
        
        # 5. æ¥æºç±»å‹åŠ æƒ
        source_weights = {
            "QueryEnt": 1.0,
            "EntBridge": 0.90,
            "RelPath": 0.85,
            "SummaryDrill": 0.85,
            "Sem": 0.80,
            "VectorJump": 0.75,
            "Seq": 0.65,
        }
        title = node.get("title", "")
        source_type = title if title in source_weights else "EntBridge"
        source_weight = source_weights.get(source_type, 0.75)
        
        # åŠ æƒèåˆ (åŠ¨æ€è°ƒæ•´ï¼šPPR æœ‰æ•ˆæ—¶ä½¿ç”¨ï¼Œå¦åˆ™å°†æƒé‡åˆ†é…ç»™å…¶ä»–ä¿¡å·)
        if ppr_available:
            trust_score = (
                0.50 * reranker_norm +
                0.10 * ppr_score +
                0.15 * entity_coverage +
                0.12 * path_penalty +
                0.13 * source_weight
            )
        else:
            # PPR æ— æ•ˆæ—¶ï¼Œå›é€€åˆ°åŸå§‹æƒé‡
            trust_score = (
                0.55 * reranker_norm +
                0.15 * entity_coverage +
                0.15 * path_penalty +
                0.15 * source_weight
            )
        
        return trust_score
    
    def search(
        self, 
        user_query: str, 
        doc_cache: Dict, 
        beam_width: int = DEFAULT_BEAM_WIDTH, 
        max_hops: int = DEFAULT_MAX_HOPS,
        doc_filter: Set[str] = None
    ) -> Dict:
        """
        è‡ªé€‚åº”å¤šè·³æ£€ç´¢ (Adaptive Multi-hop Search)
        
        å‚æ•°:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            doc_cache: æ–‡æ¡£ç¼“å­˜
            beam_width: Beam å®½åº¦
            max_hops: æœ€å¤§è·³æ•°
            doc_filter: é™åˆ¶æ£€ç´¢èŒƒå›´çš„æ–‡æ¡£ ID é›†åˆï¼ˆç”¨äº HotpotQA-Dist è®¾ç½®ï¼‰
                        å¦‚æœä¸º Noneï¼Œåˆ™ä¸é™åˆ¶ï¼ˆç”¨äº HotpotQA-Full è®¾ç½®ï¼‰
        
        è¿”å›: {"nodes": é€‰ä¸­çš„èŠ‚ç‚¹åˆ—è¡¨, "best_path": æœ€ä½³è·¯å¾„}
        """
        filter_info = f", Filter: {len(doc_filter)} docs" if doc_filter else ""
        print(f"ğŸ” Starting Adaptive Search (Max Hops: {max_hops}, Beam: {beam_width}{filter_info})")

        def get_doc_title(doc_id: str) -> str:
            if not doc_cache:
                return ""
            return doc_cache.get(doc_id, {}).get("title", "")
        
        # é‡ç½®ç¼“å­˜
        self.reset_cache()
        
        # æå–é—®é¢˜å®ä½“
        query_entities = self.entity_extractor.extract_query_entities(user_query)
        
        # è®¡ç®— PPR åˆ†æ•°ï¼ˆä»¥ query_entities ä¸ºç§å­ï¼‰
        self._ppr_scores = self._build_graph_and_compute_ppr(query_entities)
        
        # --- 1. è‡ªé€‚åº”æœç´¢åŸŸç­–ç•¥ (Context-Aware Retrieval Strategy) ---
        initial_candidates = []
        is_small_space = False  # æ ‡è®°æ˜¯å¦ä¸ºå°ç©ºé—´æ¨¡å¼ï¼ˆç”¨äºåç»­ force_keepï¼‰
        
        if doc_filter is not None:
            filter_size = len(doc_filter)
            
            if filter_size <= SMALL_SPACE_THRESHOLD:
                # === æ¨¡å¼A: å—é™å°ç©ºé—´ (Constrained Small Space) ===
                # ç­–ç•¥ï¼šå…¨é‡åŠ è½½ + Rerankï¼ˆç²¾åº¦ä¼˜å…ˆï¼‰
                # åœ¨ 10 ä¸ªæ–‡æ¡£ä¸­ï¼Œå‘é‡æ£€ç´¢å®¹æ˜“å› è¯­ä¹‰æ¼‚ç§»ä¸¢å¤±æ¡¥æ¥æ–‡æ¡£
                # ç›´æ¥åŠ è½½å…¨éƒ¨ï¼Œè®© Cross-Encoder Reranker ç²¾ç¡®æ’åº
                is_small_space = True
                print(f"  ğŸ“‹ [å°ç©ºé—´æ¨¡å¼] å…¨é‡åŠ è½½ doc_filter ({filter_size} docs)")
                
                for doc_id in doc_filter:
                    if doc_id in doc_cache:
                        doc_data = doc_cache[doc_id]
                        doc_title = doc_data.get("title", "")
                        doc_text = doc_data.get("text", "")
                        if doc_text:
                            initial_candidates.append({
                                "id": doc_id,
                                "text": doc_text,
                                "title": doc_title,
                                "doc_title": doc_title,
                                "path_history": [],
                                "context_str": "",
                                "hop_depth": 0,
                                "path_doc_titles": [doc_title] if doc_title else []
                            })
            else:
                # === æ¨¡å¼B: å—é™å¤§ç©ºé—´ (Constrained Large Space) ===
                # ç­–ç•¥ï¼šå‘é‡æ£€ç´¢ï¼Œä½†åº”ç”¨ doc_filter è¿‡æ»¤
                print(f"  ğŸ” [å¤§ç©ºé—´æ¨¡å¼] å‘é‡æ£€ç´¢ + doc_filter è¿‡æ»¤ ({filter_size} docs)")
                
                try:
                    # Summary-Guided æ£€ç´¢
                    summary_candidates = self.vector_store.summary_guided_retrieval(
                        user_query, self.graph_store, top_k=beam_width * 2
                    )
                    for sc in summary_candidates:
                        if sc["id"] in doc_filter:
                            doc_title = get_doc_title(sc["id"])
                            initial_candidates.append({
                                "id": sc["id"],
                                "text": sc["text"],
                                "title": sc.get("title", "SummaryDrill"),
                                "doc_title": doc_title,
                                "path_history": [],
                                "context_str": "",
                                "hop_depth": 0,
                                "path_doc_titles": [doc_title] if doc_title else []
                            })
                    
                    # æ™®é€šå‘é‡æ£€ç´¢è¡¥å……
                    seed_docs = self.vector_store.similarity_search_with_score(user_query, k=beam_width * 3)
                    for doc, _ in seed_docs:
                        d_id = doc.metadata.get("doc_id")
                        if d_id not in doc_filter:
                            continue
                        if any(c["id"] == d_id for c in initial_candidates):
                            continue
                        doc_title = get_doc_title(d_id)
                        initial_candidates.append({
                            "id": d_id,
                            "text": doc.page_content,
                            "title": doc.metadata.get("title", ""),
                            "doc_title": doc_title,
                            "path_history": [],
                            "context_str": "",
                            "hop_depth": 0,
                            "path_doc_titles": [doc_title] if doc_title else []
                        })
                except Exception as e:
                    print(f"âŒ Large Space Search Error: {e}")
        else:
            # === æ¨¡å¼C: å…¨å¼€æ”¾ç©ºé—´ (Open Space - Fullwiki) ===
            # ç­–ç•¥ï¼šå…¨åº“ ANN æ£€ç´¢ï¼ˆæ•ˆç‡ä¼˜å…ˆï¼‰
            print(f"  ğŸŒ [å…¨å¼€æ”¾æ¨¡å¼] å…¨åº“å‘é‡æ£€ç´¢")
            
            # Summary-Guided Top-Down Retrieval
            summary_candidates = self.vector_store.summary_guided_retrieval(
                user_query, self.graph_store, top_k=beam_width
            )
            for sc in summary_candidates:
                doc_title = get_doc_title(sc["id"])
                initial_candidates.append({
                    "id": sc["id"],
                    "text": sc["text"],
                    "title": sc.get("title", "SummaryDrill"),
                    "doc_title": doc_title,
                    "path_history": [],
                    "context_str": "",
                    "hop_depth": 0,
                    "path_doc_titles": [doc_title] if doc_title else []
                })
            
            # æ™®é€šå‘é‡æ£€ç´¢è¡¥å……
            try:
                seed_docs = self.vector_store.similarity_search_with_score(user_query, k=beam_width * 2)
                
                for doc, _ in seed_docs:
                    d_id = doc.metadata.get("doc_id")
                    d_type = doc.metadata.get("type")
                    
                    if any(c["id"] == d_id for c in initial_candidates):
                        continue
                    
                    if d_type == "summary":
                        children = self.graph_store.get_summary_children(d_id)
                        for child in children:
                            if not any(c["id"] == child["id"] for c in initial_candidates):
                                doc_title = get_doc_title(child["id"])
                                initial_candidates.append({
                                    "id": child["id"], 
                                    "text": child["text"], 
                                    "title": doc_cache.get(child["id"], {}).get("title", ""),
                                    "doc_title": doc_title,
                                    "path_history": [],
                                    "context_str": "",
                                    "hop_depth": 0,
                                    "path_doc_titles": [doc_title] if doc_title else []
                                })
                    else:
                        doc_title = get_doc_title(d_id)
                        initial_candidates.append({
                            "id": d_id, 
                            "text": doc.page_content, 
                            "title": doc.metadata.get("title", ""),
                            "doc_title": doc_title,
                            "path_history": [],
                            "context_str": "",
                            "hop_depth": 0,
                            "path_doc_titles": [doc_title] if doc_title else []
                        })
            except Exception as e:
                print(f"âŒ Open Space Search Error: {e}")
                if not initial_candidates:
                    return {"nodes": [], "best_path": ""}

        if not initial_candidates:
            return {"nodes": [], "best_path": ""}

        # --- 2. åˆå§‹æ‰“åˆ† ---
        frontier = initial_candidates
        visited_ids = set()
        final_selected_nodes = {}
        
        pairs = [[user_query, c["text"]] for c in frontier]
        reranker_scores = self._get_reranker_scores(pairs)
        
        for i, node in enumerate(frontier):
            node["reranker_score"] = reranker_scores[i]
            node["trust_score"] = self.compute_trust_score(
                node, query_entities, reranker_scores[i], node.get("hop_depth", 0)
            )
            node["score"] = node["trust_score"]
            node["path_history"] = [f"Start -> '{node['title']}'"]
            node["context_str"] = f"[{node['title']}] {node['text']}"
            if "path_doc_titles" not in node:
                doc_title = node.get("doc_title", "")
                node["path_doc_titles"] = [doc_title] if doc_title else []

        frontier.sort(key=lambda x: x["score"], reverse=True)
        # åœ¨å°ç©ºé—´æ¨¡å¼ä¸‹ï¼Œä¸ºäº†ä¸é”™è¿‡ä»»ä½•çº¿ç´¢ï¼Œæˆ‘ä»¬æ”¾å®½ Beam Width
        if is_small_space:
            # ç¡®ä¿æ‰€æœ‰åˆå§‹å€™é€‰éƒ½è¿›å…¥å›¾è°±æ¨ç†ï¼Œä½†ä¸è¶…è¿‡ SMALL_SPACE_THRESHOLD çš„ä¸Šé™
            effective_beam_width = min(len(frontier), SMALL_SPACE_THRESHOLD)
            frontier = frontier[:effective_beam_width]
        else:
            frontier = frontier[:beam_width]

        # --- 3. è¿­ä»£æ‰©å±• ---
        step = 0
        while step < max_hops and frontier:
            step += 1
            print(f"--- Step {step} (Frontier Size: {len(frontier)}) ---")
            
            current_best_node = frontier.pop(0)
            
            if current_best_node["id"] in visited_ids:
                continue
            
            visited_ids.add(current_best_node["id"])
            
            # å¯ä¿¡åº¦å‰ªæ
            should_keep = False
            if current_best_node["score"] >= TRUST_THRESHOLD:
                should_keep = True
            elif is_small_space and len(final_selected_nodes) < MIN_CANDIDATES_KEEP:
                # [å°ç©ºé—´æ¨¡å¼] å¼ºåˆ¶ä¿ç•™å‰ K ä¸ªï¼Œé˜²æ­¢è¢«é˜ˆå€¼å®Œå…¨è¯¯æ€
                should_keep = True
                print(f"  ğŸ›¡ï¸ Force Keep: {current_best_node['title']} (Trust: {current_best_node['score']:.3f} < Threshold)")

            if should_keep:
                final_selected_nodes[current_best_node["id"]] = current_best_node
                if current_best_node["score"] >= TRUST_THRESHOLD:
                    print(f"  âœ… Selected: {current_best_node['title']} (Trust: {current_best_node['score']:.3f})")
            else:
                print(f"  ğŸ—‘ï¸ Pruned: {current_best_node['title']} (Low Trust: {current_best_node['score']:.3f})")
                continue

            # æ‰©å±•é‚»å±…
            neighbors_map = self.graph_store.expand_node(
                current_best_node["id"], visited_ids, query_entities
            )
            
            # [æ–¹æ¡ˆB] Hybrid Retrieval
            if len(neighbors_map) < 3:
                hybrid_candidates = self.vector_store.hybrid_retrieval(
                    user_query, 
                    current_best_node["context_str"], 
                    visited_ids,
                    top_k=5
                )
                for hc in hybrid_candidates:
                    if hc["id"] not in neighbors_map:
                        neighbors_map[hc["id"]] = {"text": hc["text"], "title": hc["title"]}
            
            if not neighbors_map:
                continue

            # å‡†å¤‡æ–°å€™é€‰
            new_candidates = []
            current_context = current_best_node["context_str"][-1000:].replace("\n", " ")
            rerank_query = f"{user_query} [Context: {current_context}]"

            # Vector Jump
            try:
                vector_candidates = self.vector_store.similarity_search_with_score(rerank_query, k=beam_width)
                
                for v_doc, v_score in vector_candidates:
                    v_id = v_doc.metadata.get("doc_id")
                    if v_id in visited_ids:
                        continue
                    
                    # åº”ç”¨æ–‡æ¡£è¿‡æ»¤å™¨ (HotpotQA-Dist è®¾ç½®)
                    if doc_filter and v_id not in doc_filter:
                        continue
                    
                    doc_title = get_doc_title(v_id) or v_doc.metadata.get("title", "")
                    path_doc_titles = list(current_best_node.get("path_doc_titles", []))
                    if doc_title:
                        path_doc_titles.append(doc_title)

                    v_node = {
                        "id": v_id,
                        "text": v_doc.page_content,
                        "title": "VectorJump",
                        "doc_title": doc_title,
                        "path_history": current_best_node["path_history"] + [f"-> [VectorJump] '{v_doc.metadata.get('title', '')}'"],
                        "context_str": current_best_node["context_str"] + f"\n[{v_doc.metadata.get('title', '')}] {v_doc.page_content}",
                        "hop_depth": current_best_node.get("hop_depth", 0) + 1,
                        "path_doc_titles": path_doc_titles,
                    }
                    new_candidates.append(v_node)
            except Exception as e:
                print(f"âš ï¸ Vector Expansion Error: {e}")

            # Graph Expansion
            current_hop = current_best_node.get("hop_depth", 0) + 1
            for n_id, n_data in neighbors_map.items():
                if n_id in visited_ids:
                    continue
                
                # åº”ç”¨æ–‡æ¡£è¿‡æ»¤å™¨ (HotpotQA-Dist è®¾ç½®)
                if doc_filter and n_id not in doc_filter:
                    continue

                doc_title = get_doc_title(n_id)
                path_doc_titles = list(current_best_node.get("path_doc_titles", []))
                if doc_title:
                    path_doc_titles.append(doc_title)

                new_node = {
                    "id": n_id,
                    "text": n_data["text"],
                    "title": n_data["title"],
                    "doc_title": doc_title,
                    "path_history": current_best_node["path_history"] + [f"-> '{n_data['title']}'"],
                    "context_str": current_best_node["context_str"] + f"\n[{n_data['title']}] {n_data['text']}",
                    "hop_depth": current_hop,
                    "path_doc_titles": path_doc_titles,
                }
                new_candidates.append(new_node)

            if not new_candidates:
                continue

            # é™åˆ¶å€™é€‰æ•°é‡ï¼Œå‡å°‘ Reranker è°ƒç”¨å¼€é”€
            MAX_CANDIDATES_PER_HOP = beam_width * 3  # æœ€å¤š 9 ä¸ªå€™é€‰
            if len(new_candidates) > MAX_CANDIDATES_PER_HOP:
                new_candidates = new_candidates[:MAX_CANDIDATES_PER_HOP]

            # æ‰¹é‡æ‰“åˆ† (å¸¦ç¼“å­˜)
            rerank_pairs = [[rerank_query, node["text"]] for node in new_candidates]
            new_reranker_scores = self._get_reranker_scores(rerank_pairs)

            for i, node in enumerate(new_candidates):
                node["reranker_score"] = new_reranker_scores[i]
                node["trust_score"] = self.compute_trust_score(
                    node, query_entities, new_reranker_scores[i], node.get("hop_depth", 0)
                )
                node["score"] = node["trust_score"]
            
            frontier.extend(new_candidates)
            frontier.sort(key=lambda x: x["score"], reverse=True)
            frontier = frontier[:beam_width * 2]

        # --- 4. è¿”å›ç»“æœ ---
        # Fallback æœºåˆ¶ï¼šå¦‚æœå›¾è°±æ¸¸èµ°ä¸€æ— æ‰€è·ï¼Œä½†åœ¨å—é™ç©ºé—´å†…ï¼ˆå¦‚ Distractorï¼‰ï¼Œ
        # æˆ‘ä»¬ä¸èƒ½äº¤ç™½å·ã€‚å¿…é¡»æŠŠåŸå§‹æ–‡æ¡£ä½œä¸ºå…œåº•è¯æ®è¿”å›ã€‚
        if not final_selected_nodes:
            if doc_filter and len(doc_filter) <= SMALL_SPACE_THRESHOLD:
                print(f"  âš ï¸ Graph search failed. Fallback: Loading all {len(doc_filter)} docs from filter.")
                fallback_nodes = []
                for doc_id in doc_filter:
                    if doc_id in doc_cache:
                        d = doc_cache[doc_id]
                        fallback_nodes.append({
                            "id": doc_id,
                            "text": d.get("text", ""),
                            "title": d.get("title", ""),
                            "score": 0.5, # èµ‹äºˆé»˜è®¤åˆ†å€¼
                            "path_history": ["Fallback (Raw Doc)"]
                        })
                
                # å†æ¬¡å°è¯• Rerank æ’åºï¼Œé€‰å‡ºæœ€å¥½çš„
                if fallback_nodes:
                    pairs = [[user_query, n["text"]] for n in fallback_nodes]
                    scores = self._get_reranker_scores(pairs)
                    for i, node in enumerate(fallback_nodes):
                        node["score"] = scores[i]
                    
                    fallback_nodes.sort(key=lambda x: x["score"], reverse=True)
                    # åªå– Top-Beam ä½œä¸ºè¯æ®ï¼Œé¿å…è¿‡å¤šå™ªéŸ³
                    final_selected_nodes = {n["id"]: n for n in fallback_nodes[:beam_width]}
                    
            if not final_selected_nodes:
                return {"nodes": [], "best_path": ""}

        sorted_evidence = sorted(final_selected_nodes.values(), key=lambda x: x["score"], reverse=True)
        best_path_str = " -> ".join(sorted_evidence[0]["path_history"])
        
        return {
            "nodes": sorted_evidence,
            "best_path": best_path_str,
            "best_path_doc_titles": sorted_evidence[0].get("path_doc_titles", []),
        }
