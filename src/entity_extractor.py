"""
å®ä½“ä¸å…³ç³»æŠ½å–æ¨¡å—
- GLiNER: å®ä½“æŠ½å–
- REBEL: å…³ç³»æŠ½å–
"""
from __future__ import annotations

import re
from typing import List, Dict

import torch
import flair
from gliner import GLiNER
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

from src.config import (
    DEVICE, GLINER_MODEL, REBEL_MODEL, 
    ENTITY_LABELS, ENTITY_THRESHOLD, BATCH_SIZE,
    ENTITY_STOPWORDS, MIN_ENTITY_LENGTH
)
from util.text_utils import normalize_entity


class EntityExtractor:
    """åŒæ¨¡å‹å®ä½“ä¸å…³ç³»æŠ½å–å™¨"""
    
    def __init__(self):
        print(f"ğŸ“¦ åŠ è½½å®ä½“æ¨¡å‹: {GLINER_MODEL}")
        self.entity_model = GLiNER.from_pretrained(GLINER_MODEL)
        if DEVICE == "cuda":
            self.entity_model.to("cuda")
        
        print(f"ğŸ“¦ åŠ è½½å…³ç³»æŠ½å–æ¨¡å‹: {REBEL_MODEL}")
        self.rebel_tokenizer = AutoTokenizer.from_pretrained(REBEL_MODEL)
        self.rebel_model = AutoModelForSeq2SeqLM.from_pretrained(REBEL_MODEL)
        if DEVICE == "cuda":
            self.rebel_model.to("cuda")
        self.rebel_model.eval()
    
    @staticmethod
    def normalize_entity(entity_text: str) -> str:
        """å§”æ‰˜ç»™ util.text_utils"""
        return normalize_entity(entity_text)
    
    def _should_filter_entity(self, entity_text: str, entity_label: str = None, is_query: bool = False) -> bool:
        """
        åˆ¤æ–­å®ä½“æ˜¯å¦åº”è¢«è¿‡æ»¤ï¼ˆåœ¨å½’ä¸€åŒ–ä¹‹ååˆ¤æ–­ï¼‰
        è¿”å› True è¡¨ç¤ºåº”è¯¥è¿‡æ»¤æ‰
        
        å‚æ•°:
            entity_text: åŸå§‹å®ä½“æ–‡æœ¬
            entity_label: å®ä½“ç±»å‹ï¼ˆå¯é€‰ï¼Œç”¨äºæœªæ¥æ‰©å±•ï¼‰
            is_query: æ˜¯å¦ä¸ºæŸ¥è¯¢åœºæ™¯ï¼ˆæŸ¥è¯¢åœºæ™¯ä¸‹æ”¾å®½è¿‡æ»¤ï¼Œä¿ç•™æ›´å¤šæ¡¥æ¥å®ä½“ï¼‰
        """
        # å…ˆå½’ä¸€åŒ–
        normalized = self.normalize_entity(entity_text)
        
        # 1. ç©ºå€¼è¿‡æ»¤ï¼ˆå§‹ç»ˆç”Ÿæ•ˆï¼‰
        if not normalized:
            return True
        
        # 2. é•¿åº¦è¿‡æ»¤ï¼ˆæŸ¥è¯¢åœºæ™¯ä¸‹æ”¾å®½ï¼šå…è®¸é•¿åº¦ >= 1ï¼‰
        min_len = 1 if is_query else MIN_ENTITY_LENGTH
        if len(normalized) < min_len:
            return True
        
        # 3. åœç”¨è¯è¿‡æ»¤ï¼ˆæŸ¥è¯¢åœºæ™¯ä¸‹è·³è¿‡ï¼Œä¿ç•™æ‰€æœ‰ç”¨æˆ·æ„å›¾è¯ï¼‰
        if not is_query and normalized in ENTITY_STOPWORDS:
            return True
        
        # 4. çº¯æ•°å­—è¿‡æ»¤ï¼ˆä½†ä¿ç•™4ä½å¹´ä»½æ ¼å¼ï¼Œå¦‚ 1990, 2000ï¼‰
        # æŸ¥è¯¢åœºæ™¯ä¸‹ä¿ç•™æ‰€æœ‰æ•°å­—ï¼ˆç”¨æˆ·å¯èƒ½é—® "in 2015..."ï¼‰
        if not is_query and normalized.isdigit():
            if len(normalized) != 4:  # éå¹´ä»½çš„çº¯æ•°å­—è¿‡æ»¤æ‰
                return True
        
        return False
    
    def extract_entities(self, text: str) -> Dict[str, str]:
        """
        ä½¿ç”¨ GLiNER æŠ½å–å®ä½“
        è¿”å›: {å½’ä¸€åŒ–å®ä½“å: å®ä½“ç±»å‹}
        """
        try:
            # æˆªæ–­è¿‡é•¿æ–‡æœ¬ï¼Œé˜²æ­¢ OOM
            text = text[:3000]
            
            with torch.no_grad():
                ents = self.entity_model.predict_entities(
                    text, ENTITY_LABELS, threshold=ENTITY_THRESHOLD
                )
            # å½’ä¸€åŒ– + è¿‡æ»¤ + å»é‡
            unique_ents = {
                self.normalize_entity(e["text"]): e["label"] 
                for e in ents 
                if self.normalize_entity(e["text"]) 
                and not self._should_filter_entity(e["text"], e["label"])
            }
            return unique_ents
        except Exception as e:
            print(f"âš ï¸ Entity Extraction Error: {e}")
            return {}
    
    def extract_query_entities(self, query: str) -> List[str]:
        """
        ä»ç”¨æˆ·é—®é¢˜ä¸­æå–å®ä½“ï¼Œç”¨äºå¼•å¯¼å¤šè·³æ£€ç´¢
        """
        try:
            ents = self.entity_model.predict_entities(
                query, ENTITY_LABELS, threshold=ENTITY_THRESHOLD
            )
            # å½’ä¸€åŒ– + è¿‡æ»¤ + å»é‡
            # æŸ¥è¯¢åœºæ™¯ï¼šæ”¾å®½è¿‡æ»¤ï¼Œä¿ç•™æ›´å¤šæ¡¥æ¥å®ä½“ï¼ˆis_query=Trueï¼‰
            entity_names = list({
                self.normalize_entity(e["text"]) 
                for e in ents 
                if self.normalize_entity(e["text"])
                and not self._should_filter_entity(e["text"], e["label"], is_query=True)
            })
            if entity_names:
                print(f"ğŸ¯ Query Entities (normalized): {entity_names}")
            return entity_names
        except Exception as e:
            print(f"âš ï¸ Query Entity Extraction Error: {e}")
            return []
    
    def extract_relations(self, text: str) -> List[Dict]:
        """
        ä½¿ç”¨ REBEL æ¨¡å‹æŠ½å–å…³ç³»ä¸‰å…ƒç»„ (head, relation, tail)
        """
        relations = []
        try:
            # æˆªæ–­è¿‡é•¿æ–‡æœ¬é¿å… OOM
            text_truncated = text[:512]
            
            # Tokenize
            inputs = self.rebel_tokenizer(
                text_truncated, 
                return_tensors="pt", 
                max_length=512, 
                truncation=True
            )
            if DEVICE == "cuda":
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # Generate
            with torch.no_grad():
                outputs = self.rebel_model.generate(
                    **inputs,
                    max_length=256,
                    num_beams=3,
                    num_return_sequences=1
                )
            
            # Decode
            decoded = self.rebel_tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]
            
            # Parse REBEL output
            relations = self._parse_rebel_output(decoded)
            
        except Exception as e:
            print(f"âš ï¸ REBEL Extraction Error: {e}")
        
        return relations
    
    def extract_entities_batch(self, texts: List[str]) -> List[Dict[str, str]]:
        """
        æ‰¹é‡å®ä½“æŠ½å– (é«˜æ•ˆï¼Œæ”¯æŒåˆ†æ‰¹å¤„ç†é¿å… OOM)
        è¿”å›: [{å½’ä¸€åŒ–å®ä½“å: å®ä½“ç±»å‹}, ...]
        """
        results = []
        try:
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å… OOM
            for i in range(0, len(texts), BATCH_SIZE):
                batch_texts = texts[i:i + BATCH_SIZE]
                
                # é˜²å¾¡æ€§æˆªæ–­
                batch_texts = [t[:3000] for t in batch_texts]
                
                # GLiNER æ”¯æŒæ‰¹é‡é¢„æµ‹
                with torch.no_grad():
                    all_ents = self.entity_model.batch_predict_entities(
                        batch_texts, ENTITY_LABELS, threshold=ENTITY_THRESHOLD
                    )
                for ents in all_ents:
                    unique_ents = {
                        self.normalize_entity(e["text"]): e["label"]
                        for e in ents
                        if self.normalize_entity(e["text"])
                        and not self._should_filter_entity(e["text"], e["label"])
                    }
                    results.append(unique_ents)
        except Exception as e:
            print(f"âš ï¸ Batch Entity Extraction Error: {e}, falling back to sequential")
            # å›é€€åˆ°ä¸²è¡Œå¤„ç†
            results = []
            for text in texts:
                results.append(self.extract_entities(text))
        return results
    
    def extract_relations_batch(self, texts: List[str]) -> List[List[Dict]]:
        """
        æ‰¹é‡å…³ç³»æŠ½å– (é«˜æ•ˆï¼Œæ”¯æŒåˆ†æ‰¹å¤„ç†é¿å… OOM)
        è¿”å›: [[{source, target, type}, ...], ...]
        """
        if not texts:
            return []
        
        results = []
        try:
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å… OOM
            for i in range(0, len(texts), BATCH_SIZE):
                batch_texts = texts[i:i + BATCH_SIZE]
                
                # æˆªæ–­æ‰€æœ‰æ–‡æœ¬
                texts_truncated = [t[:512] for t in batch_texts]
                
                # æ‰¹é‡ Tokenize
                inputs = self.rebel_tokenizer(
                    texts_truncated,
                    return_tensors="pt",
                    max_length=512,
                    truncation=True,
                    padding=True  # æ‰¹å¤„ç†éœ€è¦ padding
                )
                if DEVICE == "cuda":
                    inputs = {k: v.to("cuda") for k, v in inputs.items()}
                
                # æ‰¹é‡ Generate
                with torch.no_grad():
                    outputs = self.rebel_model.generate(
                        **inputs,
                        max_length=256,
                        num_beams=3,
                        num_return_sequences=1
                    )
                
                # æ‰¹é‡ Decode
                decoded_batch = self.rebel_tokenizer.batch_decode(outputs, skip_special_tokens=False)
                
                # è§£ææ¯ä¸ªè¾“å‡º
                for decoded in decoded_batch:
                    relations = self._parse_rebel_output(decoded)
                    results.append(relations)
                
        except Exception as e:
            print(f"âš ï¸ Batch REBEL Error: {e}, falling back to sequential")
            # å›é€€åˆ°ä¸²è¡Œå¤„ç†
            results = []
            for text in texts:
                results.append(self.extract_relations(text))
        
        return results
    
    def _parse_rebel_output(self, text: str) -> List[Dict]:
        """
        è§£æ REBEL è¾“å‡ºæ ¼å¼
        æ ¼å¼: <triplet> head <subj> relation <obj> tail <triplet> ...
        """
        relations = []
        
        # æ¸…ç†ç‰¹æ®Š token
        text = text.replace("<s>", "").replace("</s>", "").replace("<pad>", "")
        
        # æŒ‰ <triplet> åˆ†å‰²
        triplets = text.split("<triplet>")
        
        for triplet in triplets:
            triplet = triplet.strip()
            if not triplet:
                continue
            
            try:
                # æå– head
                if "<subj>" in triplet:
                    head = triplet.split("<subj>")[0].strip()
                    rest = triplet.split("<subj>")[1]
                else:
                    continue
                
                # æå– relation å’Œ tail
                if "<obj>" in rest:
                    relation = rest.split("<obj>")[0].strip()
                    tail = rest.split("<obj>")[1].strip()
                else:
                    continue
                
                # å½’ä¸€åŒ–å®ä½“å
                head_norm = self.normalize_entity(head)
                tail_norm = self.normalize_entity(tail)
                
                if head_norm and tail_norm and head_norm != tail_norm:
                    relations.append({
                        "source": head_norm,
                        "target": tail_norm,
                        "type": relation.upper().replace(" ", "_")
                    })
            except Exception:
                continue
        
        return relations
