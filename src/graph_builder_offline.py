"""
ç¦»çº¿å›¾æ„å»ºæ¨¡å—ï¼šæ”¯æŒå¢é‡æ„å»ºå’Œæ—  LLM æ‘˜è¦
"""
from __future__ import annotations

from typing import List, Dict, Set

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI

from src.config import (
    LLM_MODEL, BATCH_SIZE,
    HARD_EDGE_ENTITY_TYPES, MIN_ENTITY_OCCURRENCES, MAX_ENTITY_OCCURRENCES,
    MAX_CHUNKS_PER_ENTITY_FOR_FULL_CONNECT, MAX_EDGES_PER_ENTITY,
    MIN_ENTITY_NAME_LENGTH
)
from src.entity_extractor import EntityExtractor
from src.graph_store import GraphStore


class OfflineGraphBuilder:
    """ç¦»çº¿ä¸‰å±‚å›¾è°±æ„å»ºå™¨"""
    
    def __init__(
        self, 
        entity_extractor: EntityExtractor, 
        graph_store: GraphStore, 
        vector_store,
        use_llm_summary: bool = False,
        logger = None
    ):
        self.entity_extractor = entity_extractor
        self.graph_store = graph_store
        self.vector_store = vector_store
        self.use_llm_summary = use_llm_summary
        self.logger = logger
        
        if use_llm_summary:
            self.llm = ChatOpenAI(model=LLM_MODEL, temperature=0)
        else:
            self.llm = None
        
        self.doc_cache: Dict[str, Dict] = {}

    def log(self, message: str):
        if self.logger:
            self.logger.info(message)
        else:
            print(message)
    
    def _summarize_text(self, text: str, hint: str) -> str:
        """
        æ‘˜è¦ç”Ÿæˆï¼šå¯é€‰ LLM æˆ–çº¯å¯å‘å¼
        å¯å‘å¼æ‘˜è¦æ— éœ€ LLM è°ƒç”¨ï¼Œå¤§å¹…é™ä½ç´¢å¼•æˆæœ¬
        """
        if not text:
            return ""
        
        # å¯å‘å¼æ‘˜è¦ï¼ˆæ—  LLM è°ƒç”¨ï¼‰
        if not self.use_llm_summary or len(text) < 500:
            clean_text = text.replace("\n", " ").strip()
            # æå–å‰å‡ å¥ä½œä¸ºæ‘˜è¦
            sentences = []
            for sep in ["ã€‚", ".", "ï¼", "!", "ï¼Ÿ", "?"]:
                if sep in clean_text:
                    parts = clean_text.split(sep)
                    sentences = [p.strip() + sep for p in parts[:3] if p.strip()]
                    break
            
            if sentences:
                summary = "".join(sentences)
            else:
                summary = clean_text[:300]
            
            return f"[{hint}] {summary}"
        
        # LLM æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
        try:
            from langchain_core.messages import HumanMessage
            prompt = f"è¯·ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼Œä¸»é¢˜æ˜¯'{hint}':\n{text[:2000]}"
            response = self.llm.invoke([HumanMessage(content=prompt)])
            return f"[{hint}] {response.content}"
        except Exception as e:
            self.log(f"âš ï¸ Summary LLM Error: {e}")
            return f"[{hint}] " + text[:200] + "..."
    
    def build(
        self, 
        documents: List[Dict], 
        existing_chunk_ids: Set[str] = None,
        start_idx: int = 0
    ) -> Dict[str, Dict]:
        """
        ä¸‰å±‚å›¾è°±æ„å»ºï¼ˆæ”¯æŒå¢é‡ï¼‰ï¼š
        Layer 1 (Tree): Document -> Summary(L2) -> Summary(L1) -> Chunk
        Layer 2 (Passage): Chunk <-> Chunk (NEXT, RELATED)
        Layer 3 (Entity): Chunk -> Entity, Entity -> Entity (RELATION)
        
        å‚æ•°:
            documents: æ–‡æ¡£åˆ—è¡¨ [{"title": str, "text": str}, ...]
            existing_chunk_ids: å·²å­˜åœ¨çš„ chunk ID é›†åˆï¼ˆç”¨äºå¢é‡æ„å»ºï¼‰
            start_idx: èµ·å§‹ç´¢å¼•ï¼ˆç”¨äºå¢é‡æ„å»ºï¼‰
        
        è¿”å›:
            doc_cache: {chunk_id: {"text": str, "title": str}, ...}
        """
        if not documents:
            return {}
        
        existing_chunk_ids = existing_chunk_ids or set()
        
        lc_docs: List[Document] = []
        ids: List[str] = []
        all_chunks: List[Dict] = []

        # Step 1: æ‰¹é‡é¢„å¤„ç† (æå–æ‰€æœ‰æ–‡æœ¬)
        texts = [doc.get("text", "") for doc in documents]
        titles = [doc.get("title", "") for doc in documents]
        
        self.log(f"  ğŸ“ Step 1/5: é¢„å¤„ç† {len(texts)} ä¸ªæ–‡æ¡£")
        
        # Step 2: æ‰¹é‡ Embedding (GPU é«˜æ•ˆåˆ©ç”¨)
        self.log(f"  ğŸ”¢ Step 2/5: æ‰¹é‡ Embedding...")
        embeddings = []
        for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="     Embedding"):
            batch_texts = texts[i : i + BATCH_SIZE]
            batch_embeddings = self.vector_store.embed_documents(batch_texts)
            embeddings.extend(batch_embeddings)
        
        # Step 3: æ‰¹é‡å®ä½“æŠ½å– (GPU é«˜æ•ˆåˆ©ç”¨)
        self.log(f"  ğŸ·ï¸ Step 3/5: æ‰¹é‡å®ä½“æŠ½å–...")
        all_entities = []
        for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="     Entity Extraction"):
            batch_texts = texts[i : i + BATCH_SIZE]
            # extract_entities_batch å†…éƒ¨ä¹Ÿæ”¯æŒæ‰¹é‡ï¼Œä½†ä¸ºäº†è¿›åº¦æ¡ï¼Œæˆ‘ä»¬è¿™é‡Œæ‰‹åŠ¨åˆ†æ‰¹è°ƒç”¨
            # æ³¨æ„ï¼šextract_entities_batch æœ¬èº«ä¼šå¤„ç† listï¼Œæ‰€ä»¥è¿™é‡Œè°ƒç”¨å®ƒæ²¡é—®é¢˜
            batch_entities = self.entity_extractor.extract_entities_batch(batch_texts)
            all_entities.extend(batch_entities)
        
        # Step 4: æ‰¹é‡å…³ç³»æŠ½å– (GPU é«˜æ•ˆåˆ©ç”¨) - å¸¦æ—©é€€ä¼˜åŒ–
        self.log(f"  ğŸ”— Step 4/5: æ‰¹é‡å…³ç³»æŠ½å–...")
        # åªå¯¹å®ä½“æ•° >= 2 çš„æ–‡æœ¬è¿›è¡Œå…³ç³»æŠ½å–
        texts_for_rebel = []
        rebel_indices = []
        for i, ents in enumerate(all_entities):
            if len(ents) >= 2:  # æ—©é€€ç­–ç•¥ï¼šå®ä½“å°‘äº2ä¸ªåˆ™è·³è¿‡REBEL
                texts_for_rebel.append(texts[i])
                rebel_indices.append(i)
        
        all_relations = [[] for _ in range(len(documents))]  # åˆå§‹åŒ–ç©ºåˆ—è¡¨
        if texts_for_rebel:
            self.log(f"     REBEL å¤„ç† {len(texts_for_rebel)}/{len(documents)} ä¸ªæ–‡æ¡£ (æ—©é€€ä¼˜åŒ–)")
            rebel_results = []
            for i in tqdm(range(0, len(texts_for_rebel), BATCH_SIZE), desc="     Relation Extraction"):
                batch_texts = texts_for_rebel[i : i + BATCH_SIZE]
                batch_rels = self.entity_extractor.extract_relations_batch(batch_texts)
                rebel_results.extend(batch_rels)
            
            for idx, rels in zip(rebel_indices, rebel_results):
                all_relations[idx] = rels
        
        # Step 5: ç»„è£… chunks
        self.log(f"  ğŸ“¦ Step 5/5: ç»„è£…å›¾è°±ç»“æ„...")
        for i, doc in enumerate(tqdm(documents, desc="     Assembling Chunks")):
            cid = f"chunk_{start_idx + i}"
            
            # è·³è¿‡å·²å­˜åœ¨çš„ chunkï¼ˆå¢é‡æ¨¡å¼ï¼‰
            if cid in existing_chunk_ids:
                continue
            
            text = texts[i]
            title = titles[i]

            self.doc_cache[cid] = {"text": text, "title": title}
            
            gliner_ents = [{"name": k, "type": v} for k, v in all_entities[i].items()]

            all_chunks.append({
                "doc_title": title, 
                "chunk_id": cid, 
                "text": text, 
                "embedding": embeddings[i],
                "entities": gliner_ents,
                "rebel_rels": all_relations[i],
                "prev_id": f"chunk_{start_idx + i - 1}" if i > 0 else None,
            })

            ids.append(cid)
            lc_docs.append(Document(
                page_content=text, 
                metadata={"doc_id": cid, "title": title, "type": "chunk"}
            ))

        if not all_chunks:
            self.log("  ğŸ“­ æ— æ–°æ–‡æ¡£éœ€è¦å¤„ç†")
            return self.doc_cache

        # Step 6: æ„å»ºæ‘˜è¦æ ‘
        self.log(f"  ğŸŒ³ æ„å»ºæ‘˜è¦æ ‘...")
        docs_map: Dict[str, List[Dict]] = {}
        for c in all_chunks:
            docs_map.setdefault(c["doc_title"], []).append(c)

        summary_nodes_batch = []
        summary_rels_batch = []
        
        for title, chunks in tqdm(docs_map.items(), desc="     Building Summary Tree"):
            if not chunks:
                continue

            # Level 1 Summaries (èšç±»)
            vecs = np.array([c["embedding"] for c in chunks])
            n_clusters = max(1, min(len(chunks) // 3, 5))
            try:
                kmeans = KMeans(n_clusters=n_clusters, n_init="auto").fit(vecs)
                labels = kmeans.labels_
            except Exception:
                labels = [0] * len(chunks)

            l1_summaries = []
            for c_idx in range(n_clusters):
                cluster_chunks = [chunks[j] for j, lbl in enumerate(labels) if lbl == c_idx]
                if not cluster_chunks:
                    continue
                
                cluster_text = " ".join([c["text"] for c in cluster_chunks])
                summary_text = self._summarize_text(cluster_text, hint=title)
                summary_id = f"summary_l1_{title}_{c_idx}"
                
                l1_summaries.append({"id": summary_id, "text": summary_text, "level": 1, "title": title})
                summary_nodes_batch.append({"id": summary_id, "text": summary_text, "level": 1, "doc_title": title})

                for chunk in cluster_chunks:
                    summary_rels_batch.append({
                        "source": summary_id, 
                        "target": chunk["chunk_id"], 
                        "type": "CONTAINS_CHUNK"
                    })
                
                ids.append(summary_id)
                lc_docs.append(Document(
                    page_content=summary_text, 
                    metadata={"doc_id": summary_id, "title": title, "type": "summary"}
                ))

            # Level 2 Summary (Root)
            if len(l1_summaries) > 1:
                l1_summary_text = "\n".join([s["text"] for s in l1_summaries])
                l2_summary_text = self._summarize_text(l1_summary_text, hint=f"Overall summary for {title}")
                l2_summary_id = f"summary_l2_{title}"

                summary_nodes_batch.append({
                    "id": l2_summary_id, 
                    "text": l2_summary_text, 
                    "level": 2, 
                    "doc_title": title
                })
                for l1_node in l1_summaries:
                    summary_rels_batch.append({
                        "source": l2_summary_id, 
                        "target": l1_node["id"], 
                        "type": "CONTAINS_SUMMARY"
                    })

                ids.append(l2_summary_id)
                lc_docs.append(Document(
                    page_content=l2_summary_text, 
                    metadata={"doc_id": l2_summary_id, "title": title, "type": "summary"}
                ))

        # Step 7: å†™å…¥å‘é‡åº“
        self.log(f"  ğŸ’¾ å†™å…¥å‘é‡åº“: {len(lc_docs)} ä¸ªæ–‡æ¡£")
        self.vector_store.add_documents(lc_docs, ids=ids)

        # Step 8: è®¡ç®—è¯­ä¹‰è¾¹ (Top-K KNN è€Œéé˜ˆå€¼æ³•)
        self.log(f"  ğŸ”— è®¡ç®—è¯­ä¹‰è¾¹...")
        semantic_rels = []
        TOP_K_NEIGHBORS = 3  # æ¯ä¸ªèŠ‚ç‚¹æœ€å¤šè¿æ¥ K ä¸ªæœ€ç›¸ä¼¼çš„é‚»å±…
        MIN_SIM_THRESHOLD = 0.5  # æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé¿å…å™ªå£°è¾¹
        
        if len(all_chunks) > 1:
            mat = np.array([c["embedding"] for c in all_chunks])
            sim_mat = cosine_similarity(mat)
            
            # å¯¹æ¯ä¸ªèŠ‚ç‚¹ï¼Œæ‰¾ Top-K æœ€ç›¸ä¼¼çš„é‚»å±…
            for i in tqdm(range(len(all_chunks)), desc="     Semantic Edges"):
                # è·å–è¯¥èŠ‚ç‚¹ä¸æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹çš„ç›¸ä¼¼åº¦
                similarities = sim_mat[i]
                # æ’é™¤è‡ªå·±ï¼Œè·å– Top-K ç´¢å¼•
                top_k_indices = np.argsort(similarities)[::-1][1:TOP_K_NEIGHBORS+1]
                
                for j in top_k_indices:
                    score = float(similarities[j])
                    # åªæ·»åŠ è¶…è¿‡æœ€ä½é˜ˆå€¼çš„è¾¹ï¼Œä¸”é¿å…é‡å¤ï¼ˆi < jï¼‰
                    if score >= MIN_SIM_THRESHOLD and i < j:
                        semantic_rels.append({
                            "source": all_chunks[i]["chunk_id"],
                            "target": all_chunks[j]["chunk_id"],
                            "score": score,
                        })

        # Step 8.5: è®¡ç®—å®ä½“å…±ç°ç¡¬è¾¹ (Entity Co-occurrence Hard Edges)
        self.log(f"  ğŸ”— è®¡ç®—å®ä½“å…±ç°ç¡¬è¾¹...")
        hard_edges = []
        
        # Step 1: å»ºç«‹å€’æ’ç´¢å¼• (entity_name -> [chunk_id, ...])
        entity_to_chunks = {}  # entity_name (normalized) -> [chunk_id, ...]
        
        for chunk in all_chunks:
            for ent in chunk["entities"]:
                ent_name = ent["name"].lower()  # å½’ä¸€åŒ–å®ä½“å
                ent_type = ent["type"]
                
                # ç±»å‹è¿‡æ»¤ï¼šåªè€ƒè™‘å¼ºç±»å‹å®ä½“
                if ent_type not in HARD_EDGE_ENTITY_TYPES:
                    continue
                
                # é•¿åº¦è¿‡æ»¤ï¼šé¿å…çŸ­åè¯¯è¿
                if len(ent_name) < MIN_ENTITY_NAME_LENGTH:
                    continue
                
                entity_to_chunks.setdefault(ent_name, []).append(chunk["chunk_id"])
        
        # Step 2: ç”Ÿæˆç¡¬è¾¹ï¼ˆå¸¦é‡‡æ ·ç­–ç•¥é˜²æ­¢è¾¹çˆ†ç‚¸ï¼‰
        entity_stats = {
            "total": len(entity_to_chunks),
            "filtered_low": 0,
            "filtered_high": 0,
            "used": 0,
            "full_connect": 0,
            "sampled": 0
        }
        
        import random
        random.seed(42)  # å¯å¤ç°çš„éšæœºé‡‡æ ·
        
        for ent_name, chunk_ids in tqdm(entity_to_chunks.items(), desc="     Hard Edges"):
            # å»é‡ + æ’åºï¼ˆä¿è¯å¯å¤ç°æ€§ï¼Œset é¡ºåºåœ¨ä¸åŒ Python ç‰ˆæœ¬/è¿è¡Œä¸­ä¸ä¸€è‡´ï¼‰
            unique_chunk_ids = sorted(list(set(chunk_ids)))
            n = len(unique_chunk_ids)
            
            # é¢‘ç‡è¿‡æ»¤
            if n < MIN_ENTITY_OCCURRENCES:
                entity_stats["filtered_low"] += 1
                continue
            if n > MAX_ENTITY_OCCURRENCES:
                entity_stats["filtered_high"] += 1
                continue
            
            entity_stats["used"] += 1
            
            # å†³å®šè¿æ¥ç­–ç•¥
            if n <= MAX_CHUNKS_PER_ENTITY_FOR_FULL_CONNECT:
                # å…¨è¿æ¥ï¼šå½“æ–‡æ¡£æ•° <= 20 æ—¶ï¼Œæ‰€æœ‰æ–‡æ¡£ä¸¤ä¸¤è¿æ¥
                entity_stats["full_connect"] += 1
                for i in range(n):
                    for j in range(i + 1, n):
                        hard_edges.append({
                            "source": unique_chunk_ids[i],
                            "target": unique_chunk_ids[j],
                            "score": 1.0,  # ç¡¬è¾¹æ»¡åˆ†
                        })
            else:
                # é‡‡æ ·è¿æ¥ï¼šå½“æ–‡æ¡£æ•° > 20 æ—¶ï¼Œé™åˆ¶è¾¹æ•°ä¸Šé™
                entity_stats["sampled"] += 1
                # ç­–ç•¥ï¼šæ¯ä¸ª chunk éšæœºè¿æ¥ k ä¸ªå…¶ä»– chunks
                k = min(5, n - 1)  # æ¯ä¸ªèŠ‚ç‚¹æœ€å¤šè¿æ¥ 5 ä¸ªé‚»å±…
                sampled_pairs = set()
                
                for i in range(n):
                    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹éšæœºé€‰æ‹© k ä¸ªé‚»å±…
                    neighbors = random.sample([j for j in range(n) if j != i], k)
                    for j in neighbors:
                        pair = tuple(sorted([i, j]))
                        if pair not in sampled_pairs:
                            sampled_pairs.add(pair)
                            if len(sampled_pairs) >= MAX_EDGES_PER_ENTITY:
                                break
                    if len(sampled_pairs) >= MAX_EDGES_PER_ENTITY:
                        break
                
                # æ·»åŠ é‡‡æ ·å¾—åˆ°çš„è¾¹
                for pair in sampled_pairs:
                    hard_edges.append({
                        "source": unique_chunk_ids[pair[0]],
                        "target": unique_chunk_ids[pair[1]],
                        "score": 1.0,
                    })
        
        self.log(f"  ğŸ“Š å®ä½“ç»Ÿè®¡: æ€»æ•°={entity_stats['total']}, "
                 f"ä½é¢‘è¿‡æ»¤={entity_stats['filtered_low']}, "
                 f"é«˜é¢‘è¿‡æ»¤={entity_stats['filtered_high']}, "
                 f"ä½¿ç”¨={entity_stats['used']} "
                 f"(å…¨è¿æ¥={entity_stats['full_connect']}, é‡‡æ ·={entity_stats['sampled']})")
        self.log(f"  ğŸ”— ç”Ÿæˆ {len(hard_edges)} æ¡å®ä½“å…±ç°ç¡¬è¾¹ (ENTITY_BRIDGE)")

        # Step 9: å†™å…¥ Neo4j
        # æ³¨æ„ï¼šè¯­ä¹‰è¾¹ (:RELATED) å’Œç¡¬è¾¹ (:ENTITY_BRIDGE) åˆ†å¼€å†™å…¥ï¼Œé¿å… score è¦†ç›–
        self.log(f"  ğŸ’¾ å†™å…¥ Neo4j: {len(all_chunks)} chunks, {len(summary_nodes_batch)} summaries, "
                 f"{len(semantic_rels)} semantic edges, {len(hard_edges)} hard edges")
        self.graph_store.write_chunks(all_chunks)
        self.graph_store.write_summaries(summary_nodes_batch, summary_rels_batch)
        self.graph_store.write_semantic_edges(semantic_rels)
        self.graph_store.write_entity_bridge_edges(hard_edges)  # ç¡¬è¾¹ç‹¬ç«‹å†™å…¥
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_entities = sum(len(c["entities"]) for c in all_chunks)
        total_relations = sum(len(c["rebel_rels"]) for c in all_chunks)
        self.log(f"  ğŸ“Š ç»Ÿè®¡: {total_entities} å®ä½“, {total_relations} å…³ç³»")
        
        return self.doc_cache
