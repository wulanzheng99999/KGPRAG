"""
Advanced GraphRAG Engine (æ¨¡å—åŒ–ç‰ˆæœ¬)
ç‰¹æ€§ï¼š
- ç»Ÿä¸€ä¸‰å±‚ KGï¼šDocument/Topic -> Chunk -> Entity
- åŒæ¨¡å‹æŠ½å–ï¼šGLiNER (å®ä½“) + REBEL (å…³ç³»)
- å¤šè·³æ£€ç´¢ï¼šBest-First Search + å¯ä¿¡åº¦è¯„åˆ†
- æ¨¡å—åŒ–æ¶æ„ï¼šæ˜“äºç»´æŠ¤å’Œæ‰©å±•
- æ”¯æŒç¦»çº¿å»ºå›¾ + åœ¨çº¿æ£€ç´¢æ¨¡å¼
"""
from __future__ import annotations

import json
from pathlib import Path
from typing import List, Dict, Optional, Set

from langchain_openai import ChatOpenAI

from src.config import LLM_MODEL, DEFAULT_BEAM_WIDTH, DEFAULT_MAX_HOPS
from src.entity_extractor import EntityExtractor
from src.graph_store import GraphStore
from src.retriever import MultiHopRetriever


class AdvancedRAGEngine:
    """
    é«˜çº§ RAG å¼•æ“ - æ¨¡å—åŒ–ç‰ˆæœ¬
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. å†…å­˜æ¨¡å¼ (persist_dir=None): å®æ—¶å»ºå›¾ï¼Œæ•°æ®ä¸æŒä¹…åŒ–
    2. æŒä¹…åŒ–æ¨¡å¼ (persist_dir="./index"): åŠ è½½ç¦»çº¿æ„å»ºçš„ç´¢å¼•
    """
    
    def __init__(
        self, 
        persist_dir: Optional[str] = None, 
        online_mode: bool = True,
        use_llm_summary: bool = False
    ):
        """
        å‚æ•°:
            persist_dir: æŒä¹…åŒ–ç›®å½•è·¯å¾„ï¼ŒNone åˆ™ä½¿ç”¨å†…å­˜æ¨¡å¼
            online_mode: True=ä»…åŠ è½½ç´¢å¼•ç”¨äºæ£€ç´¢ï¼ŒFalse=å¯æ„å»ºç´¢å¼•
            use_llm_summary: æ˜¯å¦ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦ï¼ˆé»˜è®¤ Falseï¼Œä½¿ç”¨å¯å‘å¼æ‘˜è¦åŠ é€Ÿï¼‰
        """
        self.persist_dir = persist_dir
        self.online_mode = online_mode
        self.use_llm_summary = use_llm_summary
        
        if persist_dir:
            print(f"ğŸš€ åˆå§‹åŒ– AdvancedRAG å¼•æ“ (æŒä¹…åŒ–æ¨¡å¼: {persist_dir})...")
        else:
            print("ğŸš€ åˆå§‹åŒ– AdvancedRAG å¼•æ“ (å†…å­˜æ¨¡å¼)...")
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.entity_extractor = EntityExtractor()
        self.graph_store = GraphStore()
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©å‘é‡å­˜å‚¨
        if persist_dir:
            from src.vector_store_persistent import PersistentVectorStore
            self.vector_store = PersistentVectorStore(persist_dir=persist_dir)
            self._load_doc_cache(persist_dir)
        else:
            from src.vector_store import VectorStore
            self.vector_store = VectorStore()
            self.doc_cache: Dict[str, Dict] = {}
        
        # å›¾æ„å»ºå™¨ï¼ˆä»…éåœ¨çº¿æ¨¡å¼éœ€è¦ï¼‰
        if not online_mode:
            from src.graph_builder_offline import OfflineGraphBuilder
            self.graph_builder = OfflineGraphBuilder(
                self.entity_extractor, 
                self.graph_store, 
                self.vector_store,
                use_llm_summary=use_llm_summary
            )
        else:
            self.graph_builder = None
        
        self.retriever = MultiHopRetriever(
            self.entity_extractor,
            self.graph_store,
            self.vector_store
        )
        
        # LLM for answer generation
        self.llm = ChatOpenAI(model=LLM_MODEL, temperature=0)
        
        print("âœ… å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _is_yes_no_question(self, query: str) -> bool:
        """
        æ£€æµ‹æ˜¯å¦ä¸º Yes/No é—®é¢˜
        ç”¨äºé€‰æ‹©ä¸“ç”¨ Prompt æ¨¡æ¿ï¼Œæé«˜æ ¼å¼æ­£ç¡®ç‡
        """
        query_lower = query.lower().strip()
        
        # 1. åŸºç¡€ Yes/No æ¨¡å¼
        yes_no_patterns = [
            query_lower.startswith("are "),
            query_lower.startswith("is "),
            query_lower.startswith("was "),
            query_lower.startswith("were "),
            query_lower.startswith("do "),
            query_lower.startswith("does "),
            query_lower.startswith("did "),
            query_lower.startswith("can "),
            query_lower.startswith("could "),
            query_lower.startswith("would "),
            query_lower.startswith("will "),
            query_lower.startswith("has "),
            query_lower.startswith("have "),
            query_lower.startswith("had "),
        ]
        if any(yes_no_patterns):
            return True

        # 2. å¢å¼ºæ¨¡å¼ï¼šæ£€æµ‹ "same" / "both" ç±»å‹çš„æ¯”è¾ƒé—®é¢˜
        # e.g., "Were Scott Derrickson and Ed Wood of the same nationality?"
        if "same" in query_lower and any(x in query_lower for x in ["nationality", "country", "type", "category", "genre", "year", "time"]):
            return True
        
        if "both" in query_lower and any(x in query_lower for x in ["are", "were", "born", "died", "from"]):
            return True

        return False
    
    def _post_process_answer(self, raw_answer: str, query: str) -> str:
        """
        é€šç”¨ç­”æ¡ˆåå¤„ç†ï¼šå¼ºåˆ¶æå– 'Answer:' åçš„å†…å®¹
        """
        answer = raw_answer.strip()
        
        # --- 1. é€šç”¨æå–é€»è¾‘ï¼šå–æœ€åä¸€ä¸ª "Answer:" ---
        # å¾ˆå¤šæ—¶å€™æ¨¡å‹ä¼šåœ¨ Reasoning ä¹‹åè¾“å‡º "Answer: xxx"ï¼Œæˆ–è€…å¤šæ¬¡è¾“å‡º
        import re
        # æŸ¥æ‰¾æ‰€æœ‰ "Answer:" (å¿½ç•¥å¤§å°å†™) çš„ä½ç½®
        # ä½¿ç”¨æ­£åˆ™æ‰¾ "Answer:" æˆ– "Final Answer:" æˆ– "ç­”æ¡ˆï¼š"
        markers = ["answer:", "final answer:", "ç­”æ¡ˆï¼š"]
        
        last_answer_content = None
        
        # ç®€å•ç­–ç•¥ï¼šæŒ‰è¡Œå€’åºæŸ¥æ‰¾
        lines = [l.strip() for l in answer.split('\n') if l.strip()]
        for line in reversed(lines):
            line_lower = line.lower()
            for marker in markers:
                if marker in line_lower:
                    # æ‰¾åˆ°äº†æ ‡è®°ï¼Œæå–æ ‡è®°åçš„å†…å®¹
                    parts = line_lower.rsplit(marker, 1) # åªåˆ†å‰²æœ€åä¸€æ¬¡å‡ºç°çš„ marker
                    if len(parts) >= 2:
                        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä»åŸå§‹ line ä¸­æå–ï¼Œä»¥ä¿ç•™å¤§å°å†™ï¼ˆè™½ç„¶ HotpotQA è¯„æµ‹ä¸æ•æ„Ÿï¼Œä½†ä¿ç•™æ›´å¥½ï¼‰
                        # é‡æ–°å®šä½ marker åœ¨åŸå§‹ line ä¸­çš„ç´¢å¼•
                        idx = line.lower().rfind(marker)
                        candidate = line[idx + len(marker):].strip()
                        if candidate:
                            last_answer_content = candidate
                            break
            if last_answer_content:
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ° Answer: æ ‡è®°ï¼Œåˆ™å…œåº•å–æœ€åä¸€è¡Œéç©ºæ–‡æœ¬
        if not last_answer_content:
            if lines:
                last_answer_content = lines[-1]
            else:
                last_answer_content = answer

        # --- 2. æ¸…æ´—æå–å‡ºçš„ç­”æ¡ˆ ---
        final_ans = last_answer_content.strip()
        
        # å»é™¤é¦–å°¾çš„æ ‡ç‚¹å’Œå¼•å· (e.g., "Yes.", "1990", **1990**)
        final_ans = re.sub(r'^["\'\*`]+|["\'\*`\.\!]+$', '', final_ans)
        
        # å»é™¤å¸¸è§çš„åºŸè¯å‰ç¼€
        # e.g., "The answer is 1990" -> "1990"
        final_ans = re.sub(r'^(the answer is |it is |that is )', '', final_ans, flags=re.I).strip()
        
        # --- 3. Yes/No æ ‡å‡†åŒ– (ä»…é’ˆå¯¹ Yes/No é—®é¢˜) ---
        if self._is_yes_no_question(query):
            ans_lower = final_ans.lower()
            if ans_lower.startswith("yes"): return "yes"
            if ans_lower.startswith("no"): return "no"
            
            # å…œåº•æ£€æµ‹ï¼ˆå¦‚æœæå–å‡ºçš„å†…å®¹è¿˜åŒ…å«å…¶ä»–è¯ï¼‰
            if "yes" in ans_lower: return "yes"
            if "no" in ans_lower: return "no"
            
        return final_ans
    
    def _load_doc_cache(self, persist_dir: str):
        """åŠ è½½æŒä¹…åŒ–çš„ doc_cache"""
        cache_path = Path(persist_dir) / "doc_cache.json"
        if cache_path.exists():
            with open(cache_path, "r", encoding="utf-8") as f:
                self.doc_cache = json.load(f)
            print(f"ğŸ“‚ åŠ è½½ {len(self.doc_cache)} ä¸ªæ–‡æ¡£ç¼“å­˜")
        else:
            self.doc_cache = {}
            print("âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£ç¼“å­˜ï¼Œè¯·å…ˆè¿è¡Œç¦»çº¿å»ºå›¾")
    
    def reset(self):
        """é‡ç½®æ‰€æœ‰å­˜å‚¨"""
        self.doc_cache = {}
        self.graph_store.reset()
        self.vector_store.reset()
    
    def load_precomputed_cache(self, cache_data: Dict):
        """
        åŠ è½½é¢„è®¡ç®—çš„ç¼“å­˜æ•°æ®ï¼ˆè·³è¿‡ GLiNER/REBEL æ¨ç†ï¼‰
        
        è¿™æ˜¯ HotpotQA è¯„æµ‹çš„æ¨èæ–¹å¼ï¼š
        1. å…ˆç”¨ scripts/precompute_hotpot.py é¢„è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å›¾è°±æ•°æ®
        2. è¯„æµ‹æ—¶ç›´æ¥åŠ è½½ç¼“å­˜ï¼Œæ— éœ€å®æ—¶æŠ½å–
        
        å‚æ•°:
            cache_data: é¢„è®¡ç®—è„šæœ¬ç”Ÿæˆçš„ç¼“å­˜å­—å…¸ï¼ŒåŒ…å«:
                - chunks: é¢„è®¡ç®—çš„ chunk æ•°æ®ï¼ˆå« embeddingã€entitiesã€relationsï¼‰
                - summaries: æ‘˜è¦èŠ‚ç‚¹
                - summary_rels: æ‘˜è¦å…³ç³»
                - semantic_edges: è¯­ä¹‰è¾¹
                - doc_cache: æ–‡æ¡£ç¼“å­˜
        """
        # 1. åŠ è½½ doc_cache
        self.doc_cache = cache_data.get("doc_cache", {})
        
        # 2. å†™å…¥ Neo4j å›¾å­˜å‚¨
        chunks = cache_data.get("chunks", [])
        summaries = cache_data.get("summaries", [])
        summary_rels = cache_data.get("summary_rels", [])
        semantic_edges = cache_data.get("semantic_edges", [])
        
        self.graph_store.write_chunks(chunks)
        self.graph_store.write_summaries(summaries, summary_rels)
        self.graph_store.write_semantic_edges(semantic_edges)
        
        # 3. å†™å…¥å‘é‡å­˜å‚¨
        from langchain_core.documents import Document
        lc_docs = []
        ids = []
        
        # æ·»åŠ  chunks
        for chunk in chunks:
            lc_docs.append(Document(
                page_content=chunk["text"],
                metadata={
                    "doc_id": chunk["chunk_id"],
                    "title": chunk["doc_title"],
                    "type": "chunk"
                }
            ))
            ids.append(chunk["chunk_id"])
        
        # æ·»åŠ  summaries
        for summary in summaries:
            lc_docs.append(Document(
                page_content=summary["text"],
                metadata={
                    "doc_id": summary["id"],
                    "title": summary["doc_title"],
                    "type": "summary"
                }
            ))
            ids.append(summary["id"])
        
        self.vector_store.add_documents(lc_docs, ids=ids)
    
    def ingest(self, documents: List[Dict]):
        """
        æ‘„å…¥æ–‡æ¡£ï¼Œæ„å»ºä¸‰å±‚å›¾è°±
        documents: [{"title": str, "text": str}, ...]
        
        æ³¨æ„ï¼šæŒä¹…åŒ–æ¨¡å¼ä¸‹å»ºè®®ä½¿ç”¨ç¦»çº¿å»ºå›¾è„šæœ¬ scripts/build_index.py
        """
        if self.graph_builder is None:
            raise RuntimeError(
                "åœ¨çº¿æ¨¡å¼ä¸‹ä¸æ”¯æŒ ingest()ï¼Œè¯·ä½¿ç”¨ç¦»çº¿å»ºå›¾è„šæœ¬:\n"
                "python scripts/build_index.py --input data/documents.json --persist_dir ./index"
            )
        self.doc_cache = self.graph_builder.build(documents)
    
    def query(
        self, 
        user_query: str, 
        beam_width: int = DEFAULT_BEAM_WIDTH, 
        max_hops: int = DEFAULT_MAX_HOPS,
        doc_filter: Set[str] = None,
        return_debug: bool = False
    ) -> str:
        """
        æŸ¥è¯¢æ¥å£
        
        å‚æ•°:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            beam_width: Beam å®½åº¦
            max_hops: æœ€å¤§è·³æ•°
            doc_filter: é™åˆ¶æ£€ç´¢èŒƒå›´çš„æ–‡æ¡£ ID é›†åˆï¼ˆç”¨äº HotpotQA-Dist è®¾ç½®ï¼‰
            return_debug: æ˜¯å¦è¿”å›æ£€ç´¢è°ƒè¯•ä¿¡æ¯
        """
        # å¤šè·³æ£€ç´¢
        search_result = self.retriever.search(
            user_query, 
            self.doc_cache,
            beam_width=beam_width, 
            max_hops=max_hops,
            doc_filter=doc_filter
        )
        
        if not search_result["nodes"]:
            answer = "I don't know."
            if return_debug:
                return answer, {"search_result": search_result}
            return answer
        
        # ç”Ÿæˆç­”æ¡ˆ
        sorted_evidence = search_result["nodes"]
        context_str = "\n\n".join([f"[{n['title']}] {n['text']}" for n in sorted_evidence])
        best_path_str = search_result["best_path"]
        
        # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ä¸åŒçš„ Prompt
        is_yes_no = self._is_yes_no_question(user_query)
        
        if is_yes_no:
            # Yes/No ä¸“ç”¨ Prompt - å…è®¸ç®€çŸ­æ¨ç†
            prompt = f"""You are a precise QA system. Answer the Yes/No question based on the context.

**INSTRUCTION**: 
- First, briefly reason about the answer (1-2 sentences max)
- Then, output your final answer as ONLY "yes" or "no" on a new line
- Format: 
  Reasoning: [brief reasoning]
  Answer: yes/no

**Context:**
{context_str}

**Question:** {user_query}
"""
        else:
            # æ™®é€šé—®é¢˜: v2.1 å¢å¼ºç¨³å¥ç‰ˆ (With Path & Anchor)
            # 1. ä¿ç•™ Reasoning Path ä»¥åˆ©ç”¨å›¾è°±ä¼˜åŠ¿
            # 2. ä¿ç•™ Answer: é”šç‚¹ä»¥è¯±å¯¼ç›´æ¥è¾“å‡º
            prompt = f"""You are a precise QA system. Answer the question based on the Context.

**Output Format (STRICTLY REQUIRED):**
- Output ONLY ONE LINE: Answer: <your answer>
- <your answer> MUST be: a single entity name, date, number, or short phrase (max 5 words)
- DO NOT include any explanation, reasoning, or sentence structure

**Reasoning Path:**
{best_path_str}

**Context:**
{context_str}

**Question:** {user_query}
**Answer:**"""
        
        raw_answer = self.llm.invoke(prompt).content
        
        # åå¤„ç†ç­”æ¡ˆ
        answer = self._post_process_answer(raw_answer, user_query)
        if return_debug:
            return answer, {"search_result": search_result}
        return answer
    
    # å…¼å®¹æ—§æ¥å£
    def query_adaptive_search(
        self, 
        user_query: str, 
        beam_width: int = DEFAULT_BEAM_WIDTH, 
        max_hops: int = DEFAULT_MAX_HOPS,
        doc_filter: Set[str] = None,
        return_debug: bool = False
    ) -> str:
        """å…¼å®¹æ—§æ¥å£"""
        return self.query(
            user_query,
            beam_width,
            max_hops,
            doc_filter,
            return_debug=return_debug
        )
    
    def close(self):
        """å…³é—­è¿æ¥"""
        self.graph_store.close()
