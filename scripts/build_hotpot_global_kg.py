"""
HotpotQA å…¨é‡å»ºå›¾è„šæœ¬ï¼šæå–æ‰€æœ‰å”¯ä¸€æ–‡æ¡£å¹¶æ„å»ºå…¨å±€ KG

å¯¹æ ‡ KG2RAG è®ºæ–‡çš„å®éªŒè®¾ç½®ï¼š
- ä» HotpotQA æ•°æ®é›†ä¸­æå–æ‰€æœ‰å”¯ä¸€æ–‡æ¡£
- æ„å»ºå…¨é‡çŸ¥è¯†å›¾è°±ï¼ˆä¸€æ¬¡æ€§ç¦»çº¿æ„å»ºï¼‰
- è¯„æµ‹æ—¶é€šè¿‡ doc_filter é™åˆ¶æ£€ç´¢èŒƒå›´

ç”¨æ³•:
    python scripts/build_hotpot_global_kg.py --input data/hotpot_dev_distractor_v1.json --persist_dir ./index
"""
import argparse
import json
import sys
import time
from pathlib import Path
from typing import Dict, List, Set

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.entity_extractor import EntityExtractor
from src.graph_store import GraphStore
from src.graph_builder_offline import OfflineGraphBuilder
from src.vector_store_persistent import PersistentVectorStore
from util.custom_logger import ExperimentLogger


def extract_unique_documents(data: List[Dict]) -> tuple:
    """
    ä» HotpotQA æ•°æ®é›†ä¸­æå–æ‰€æœ‰å”¯ä¸€æ–‡æ¡£
    
    è¿”å›:
        documents: å»é‡åçš„æ–‡æ¡£åˆ—è¡¨ [{"title": str, "text": str}, ...]
        title_to_doc_id: æ ‡é¢˜åˆ°æ–‡æ¡£IDçš„æ˜ å°„ {title: doc_id}
        sample_doc_mapping: æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ–‡æ¡£IDåˆ—è¡¨ {sample_id: [doc_id1, doc_id2, ...]}
    """
    unique_docs: Dict[str, str] = {}  # title -> text
    sample_doc_mapping: Dict[str, List[str]] = {}  # sample_id -> [doc_ids]
    
    for sample in data:
        sample_id = sample["_id"]
        context = sample["context"]
        doc_ids = []
        
        for item in context:
            title = item[0]
            sentences = item[1]
            text = " ".join(sentences)
            
            # ä½¿ç”¨ title ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆHotpotQA ä¸­ title æ˜¯å”¯ä¸€çš„ï¼‰
            if title not in unique_docs:
                unique_docs[title] = text
            
            doc_ids.append(title)  # ä½¿ç”¨ title ä½œä¸º doc_id
        
        sample_doc_mapping[sample_id] = doc_ids
    
    # è½¬æ¢ä¸ºæ–‡æ¡£åˆ—è¡¨
    documents = [{"title": title, "text": text} for title, text in unique_docs.items()]
    
    # åˆ›å»º title -> doc_id æ˜ å°„ï¼ˆè¿™é‡Œ doc_id å°±æ˜¯åœ¨ documents åˆ—è¡¨ä¸­çš„ç´¢å¼•å¯¹åº”çš„ chunk_idï¼‰
    title_to_doc_id = {doc["title"]: f"chunk_{i}" for i, doc in enumerate(documents)}
    
    # æ›´æ–° sample_doc_mapping ä½¿ç”¨ chunk_id
    sample_doc_mapping_with_chunk_ids = {}
    for sample_id, titles in sample_doc_mapping.items():
        sample_doc_mapping_with_chunk_ids[sample_id] = [title_to_doc_id[t] for t in titles]
    
    return documents, title_to_doc_id, sample_doc_mapping_with_chunk_ids


def main():
    parser = argparse.ArgumentParser(description="HotpotQA å…¨é‡å»ºå›¾è„šæœ¬")
    parser.add_argument("--input", default="data/hotpot_dev_distractor_v1.json", help="HotpotQA JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--persist_dir", default="data/hotpotqa", help="æŒä¹…åŒ–ç›®å½• (é»˜è®¤: data/hotpotqa)")
    parser.add_argument("--reset", action="store_true", help="æ¸…ç©ºç°æœ‰ç´¢å¼•åé‡å»º")
    parser.add_argument("--skip_existing", action="store_true", help="è·³è¿‡å·²å­˜åœ¨çš„æ–‡æ¡£ (æ–­ç‚¹ç»­ä¼ )")
    parser.add_argument("--use_llm_summary", action="store_true", help="ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦ (é»˜è®¤: å¯å‘å¼æ‘˜è¦)")
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ—¥å¿—
    log_dir = project_root / "logs" / "kgs"
    logger = ExperimentLogger(log_dir=str(log_dir), experiment_name="build_kg")
    
    logger.info("=" * 70)
    logger.info("ğŸš€ HotpotQA å…¨é‡å»ºå›¾å·¥å…· (å¯¹æ ‡ KG2RAG)")
    logger.info("=" * 70)
    logger.info(f"ğŸ“‚ è¾“å…¥æ–‡ä»¶: {args.input}")
    logger.info(f"ğŸ“‚ æŒä¹…åŒ–ç›®å½•: {args.persist_dir}")
    logger.info(f"âš™ï¸  ä½¿ç”¨ LLM æ‘˜è¦: {args.use_llm_summary}")
    logger.info(f"â© æ–­ç‚¹ç»­ä¼ : {args.skip_existing}")
    logger.info("=" * 70)
    
    # 1. åˆ›å»ºæŒä¹…åŒ–ç›®å½•
    persist_dir = Path(args.persist_dir)
    persist_dir.mkdir(parents=True, exist_ok=True)
    
    # 2. åŠ è½½æ•°æ®é›†
    logger.info(f"\nğŸ“„ åŠ è½½æ•°æ®é›†...")
    input_path = project_root / args.input
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    logger.info(f"âœ… åŠ è½½ {len(data)} ä¸ª QA æ ·æœ¬")
    
    # 3. æå–æ‰€æœ‰å”¯ä¸€æ–‡æ¡£
    logger.info(f"\nğŸ“‘ æå–å”¯ä¸€æ–‡æ¡£...")
    documents, title_to_doc_id, sample_doc_mapping = extract_unique_documents(data)
    logger.info(f"âœ… æå– {len(documents)} ä¸ªå”¯ä¸€æ–‡æ¡£")
    logger.info(f"   (åŸå§‹ {len(data) * 10} ä¸ªæ–‡æ¡£ï¼Œå»é‡ç‡: {1 - len(documents) / (len(data) * 10):.1%})")
    
    # 4. ä¿å­˜æ–‡æ¡£æ˜ å°„ï¼ˆè¯„æµ‹æ—¶éœ€è¦ç”¨ï¼‰
    mapping_path = persist_dir / "sample_doc_mapping.json"
    with open(mapping_path, "w", encoding="utf-8") as f:
        json.dump(sample_doc_mapping, f, ensure_ascii=False)
    logger.info(f"ğŸ’¾ ä¿å­˜æ ·æœ¬-æ–‡æ¡£æ˜ å°„: {mapping_path}")
    
    title_mapping_path = persist_dir / "title_to_doc_id.json"
    with open(title_mapping_path, "w", encoding="utf-8") as f:
        json.dump(title_to_doc_id, f, ensure_ascii=False)
    logger.info(f"ğŸ’¾ ä¿å­˜æ ‡é¢˜-æ–‡æ¡£IDæ˜ å°„: {title_mapping_path}")
    
    # 5. åˆå§‹åŒ–æ¨¡å—
    logger.info(f"\nğŸ”§ åˆå§‹åŒ–æ¨¡å—...")
    entity_extractor = EntityExtractor()
    graph_store = GraphStore()
    vector_store = PersistentVectorStore(persist_dir=str(persist_dir))
    
    graph_builder = OfflineGraphBuilder(
        entity_extractor=entity_extractor,
        graph_store=graph_store,
        vector_store=vector_store,
        use_llm_summary=args.use_llm_summary,
        logger=logger
    )
    logger.info("âœ… æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    # 6. é‡ç½®ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    existing_chunk_ids = set()
    if args.reset:
        logger.info(f"\nğŸ—‘ï¸ æ¸…ç©ºç°æœ‰ç´¢å¼•...")
        graph_store.reset()
        vector_store.reset()
        logger.info("âœ… ç´¢å¼•å·²æ¸…ç©º")
    elif args.skip_existing:
        logger.info(f"\nğŸ” æ£€æŸ¥å·²å­˜åœ¨æ–‡æ¡£...")
        existing_chunk_ids = graph_store.get_existing_chunk_ids()
        logger.info(f"âœ… å‘ç° {len(existing_chunk_ids)} ä¸ªå·²å­˜åœ¨ Chunk")
    
    # 7. æ„å»ºå…¨é‡å›¾è°±
    logger.info(f"\nğŸš€ å¼€å§‹æ„å»ºå…¨é‡å›¾è°±...")
    logger.info(f"   å…± {len(documents)} ä¸ªæ–‡æ¡£")
    start_time = time.time()
    
    doc_cache = graph_builder.build(documents, existing_chunk_ids=existing_chunk_ids)
    
    build_time = time.time() - start_time
    logger.info(f"âœ… å›¾è°±æ„å»ºå®Œæˆï¼Œè€—æ—¶: {build_time:.1f}s")
    
    # 8. ä¿å­˜ doc_cache
    cache_path = persist_dir / "doc_cache.json"
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(doc_cache, f, ensure_ascii=False)
    logger.info(f"ğŸ’¾ ä¿å­˜æ–‡æ¡£ç¼“å­˜: {cache_path}")
    
    # 9. ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"\n{'=' * 70}")
    logger.info(f"âœ… å…¨é‡å»ºå›¾å®Œæˆ!")
    logger.info(f"ğŸ“Š ç»Ÿè®¡:")
    logger.info(f"   - QA æ ·æœ¬æ•°: {len(data)}")
    logger.info(f"   - å”¯ä¸€æ–‡æ¡£æ•°: {len(documents)}")
    logger.info(f"   - æ–‡æ¡£ç¼“å­˜å¤§å°: {len(doc_cache)}")
    logger.info(f"   - æ„å»ºè€—æ—¶: {build_time:.1f}s")
    logger.info(f"   - å¹³å‡æ¯æ–‡æ¡£: {build_time / len(documents):.3f}s")
    logger.info(f"ğŸ“‚ æŒä¹…åŒ–ç›®å½•: {persist_dir.absolute()}")
    logger.info(f"\nğŸ“ ä¸‹ä¸€æ­¥:")
    logger.info(f"   è¿è¡Œè¯„æµ‹: python evaluate.py")
    logger.info("=" * 70)
    
    # 10. å…³é—­è¿æ¥
    graph_store.close()


if __name__ == "__main__":
    main()
