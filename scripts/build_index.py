"""
ç¦»çº¿å»ºå›¾è„šæœ¬ï¼šä¸€æ¬¡æ€§æ„å»ºä¸‰å±‚çŸ¥è¯†å›¾è°±å¹¶æŒä¹…åŒ–

ç”¨æ³•:
    # å…¨é‡æ„å»ºï¼ˆæ¸…ç©ºç°æœ‰ç´¢å¼•ï¼‰
    python scripts/build_index.py --input data/documents.json --persist_dir ./index --reset
    
    # å¢é‡æ„å»ºï¼ˆä¿ç•™ç°æœ‰ç´¢å¼•ï¼Œåªæ·»åŠ æ–°æ–‡æ¡£ï¼‰
    python scripts/build_index.py --input data/new_documents.json --persist_dir ./index
    
è¾“å…¥æ–‡ä»¶æ ¼å¼ (JSON):
    [
        {"title": "æ–‡æ¡£æ ‡é¢˜1", "text": "æ–‡æ¡£å†…å®¹1"},
        {"title": "æ–‡æ¡£æ ‡é¢˜2", "text": "æ–‡æ¡£å†…å®¹2"},
        ...
    ]
"""
import argparse
import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.entity_extractor import EntityExtractor
from src.graph_store import GraphStore
from src.graph_builder_offline import OfflineGraphBuilder
from src.vector_store_persistent import PersistentVectorStore


def load_documents(input_path: str) -> list:
    """åŠ è½½æ–‡æ¡£"""
    with open(input_path, "r", encoding="utf-8") as f:
        documents = json.load(f)
    
    # éªŒè¯æ ¼å¼
    if not isinstance(documents, list):
        raise ValueError("è¾“å…¥æ–‡ä»¶å¿…é¡»æ˜¯ JSON æ•°ç»„")
    
    for i, doc in enumerate(documents):
        if "text" not in doc:
            raise ValueError(f"æ–‡æ¡£ {i} ç¼ºå°‘ 'text' å­—æ®µ")
        if "title" not in doc:
            doc["title"] = f"Document_{i}"
    
    return documents


def main():
    parser = argparse.ArgumentParser(description="ç¦»çº¿å»ºå›¾è„šæœ¬")
    parser.add_argument("--input", required=True, help="è¾“å…¥æ–‡æ¡£ JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--persist_dir", default="./index", help="æŒä¹…åŒ–ç›®å½• (é»˜è®¤: ./index)")
    parser.add_argument("--reset", action="store_true", help="æ¸…ç©ºç°æœ‰ç´¢å¼•åé‡å»º")
    parser.add_argument("--use_llm_summary", action="store_true", help="ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦ (é»˜è®¤: å¯å‘å¼æ‘˜è¦)")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 32)")
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ KGPRAG ç¦»çº¿å»ºå›¾å·¥å…·")
    print("=" * 60)
    
    # 1. åŠ è½½æ–‡æ¡£
    print(f"\nğŸ“„ åŠ è½½æ–‡æ¡£: {args.input}")
    documents = load_documents(args.input)
    print(f"   å…± {len(documents)} ä¸ªæ–‡æ¡£")
    
    # 2. åˆå§‹åŒ–ç»„ä»¶
    print("\nğŸ“¦ åˆå§‹åŒ–ç»„ä»¶...")
    entity_extractor = EntityExtractor()
    graph_store = GraphStore()
    vector_store = PersistentVectorStore(persist_dir=args.persist_dir)
    
    # 3. å¤„ç† reset
    if args.reset:
        print("\nğŸ—‘ï¸ æ¸…ç©ºç°æœ‰ç´¢å¼•...")
        graph_store.reset()
        vector_store.reset()
        existing_ids = set()
    else:
        existing_ids = vector_store.get_existing_ids()
        print(f"\nğŸ“‚ ç°æœ‰ç´¢å¼•åŒ…å« {len(existing_ids)} ä¸ªæ–‡æ¡£")
    
    # 4. æ„å»ºå›¾è°±
    print("\nğŸ”¨ å¼€å§‹æ„å»ºä¸‰å±‚çŸ¥è¯†å›¾è°±...")
    builder = OfflineGraphBuilder(
        entity_extractor, 
        graph_store, 
        vector_store,
        use_llm_summary=args.use_llm_summary
    )
    
    doc_cache = builder.build(
        documents, 
        existing_chunk_ids=existing_ids,
        start_idx=len(existing_ids)
    )
    
    # 5. ä¿å­˜ doc_cache
    persist_path = Path(args.persist_dir)
    persist_path.mkdir(parents=True, exist_ok=True)
    
    cache_path = persist_path / "doc_cache.json"
    
    # å¢é‡æ¨¡å¼ï¼šåˆå¹¶ç°æœ‰ç¼“å­˜
    if cache_path.exists() and not args.reset:
        with open(cache_path, "r", encoding="utf-8") as f:
            old_cache = json.load(f)
        old_cache.update(doc_cache)
        doc_cache = old_cache
    
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(doc_cache, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 60)
    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ!")
    print(f"   æŒä¹…åŒ–ç›®å½•: {args.persist_dir}")
    print(f"   æ–‡æ¡£ç¼“å­˜: {cache_path}")
    print(f"   æ€»æ–‡æ¡£æ•°: {len(doc_cache)}")
    print("=" * 60)


if __name__ == "__main__":
    main()
